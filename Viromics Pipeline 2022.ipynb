{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CREATE\n",
    "\n",
    "# to make a directory\n",
    "mkdir directory\n",
    "\n",
    "# to make multiple directories at the same time\n",
    "mkdir directory1 directory2 directory3\n",
    "\n",
    "# to write a file\n",
    "nano filename\n",
    "\n",
    "# the best partition to use is bmh, \n",
    "# On the high2 partition: 256 CPUs and 500GB of RAM\n",
    "# On the bmh partition: 128 CPUs and 1TB of RAM\n",
    "\n",
    "----------------------------\n",
    "\n",
    "## CHANGE\n",
    "\n",
    "\n",
    "# to move files up a folder\n",
    "mv * ../\n",
    "\n",
    "# to move directory into another directory (that already exists)\n",
    "mv directorytomove directorythatexists\n",
    "\n",
    "# to \"rename\" a file (for example, rename something called \"error\" to \"err\")\n",
    "mv error err\n",
    "\n",
    "# to copy a file and rename (if in same folder)\n",
    "cp file ./filenew\n",
    "\n",
    "cp qual_filter.sh ./qual_filter2.sh\n",
    "\n",
    "# to copy a file and move to different directory\n",
    "cp file directory/\n",
    "cp file1 file2 file3 directory/\n",
    "\n",
    "----------------------------\n",
    "\n",
    "## MONITOR\n",
    "\n",
    "# to view a file\n",
    "less filename\n",
    "\n",
    "#to access personal folders (on your own windows computer)\n",
    "cd /mnt/c/\n",
    "\n",
    "# to see status of jobs (input username, mine is seuge)\n",
    "squeue -u seuge\n",
    "\n",
    "# to cancel jobs (input username, mine is seuge)\n",
    "scancel -u seuge\n",
    "\n",
    "# view size of files in human readable form\n",
    "ls -lah\n",
    "ls\n",
    "\n",
    "# counting lines in a file\n",
    "wc -l filename\n",
    "\n",
    "# list one file per line    \n",
    "ls -1\n",
    "\n",
    "# counting number of files in a directory\n",
    "ls -1 | wc -l\n",
    "\n",
    "# to see the beginning of many files in a directory\n",
    "head -5 *.txt\n",
    "# with 5 being the number lines, you can edit this with how many lines you want to see\n",
    "\n",
    "# to search for something in files\n",
    "grep -r 'path' -e 'something'\n",
    "grep -r 'home/seuge91/fire/reads/log' -e '53927615'\n",
    "\n",
    "# to count number of lines in a zipped file\n",
    "zcat 'file name' | wc -l\n",
    "\n",
    "# to view the next file in less, press “:” and then hit “n”\n",
    "\n",
    "# to list installed packages and their versions (either command seems to work just fine)\n",
    "pip list\n",
    "conda list\n",
    "\n",
    "----------------------------\n",
    "\n",
    "# DELETE\n",
    "\n",
    "# to remove a file \n",
    "rm filename\n",
    "\n",
    "# to remove all files in a folder\n",
    "rm *\n",
    "\n",
    "# to remove a directory by overriding computer\n",
    "rm -r nameofdirectory\n",
    "\n",
    "----------------------------\n",
    "\n",
    "# STOP/EXIT\n",
    "\n",
    "# exit nano by ctrl-x, Y (to save), and enter\n",
    "\n",
    "# exit less\n",
    "q\n",
    "\n",
    "# stop a command line execution\n",
    "CTRL+c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data\n",
    "\n",
    "These will be fastq.gz files, paired-end reads\n",
    "\n",
    "Downloading data from genome center https://dnatech.genomecenter.ucdavis.edu/archiving-slims-data/ (rsync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# screen to have something running while doing something else (like making copies)\n",
    "screen \n",
    "\n",
    "# move into directory you want the data to go (don't forget the dot at the end!!!)\n",
    "rsync -avL slimsdata.genomecenter.ucdavis.edu::slims/5ipm20rfhg . \n",
    "\n",
    "rsync -avL slimsdata.genomecenter.ucdavis.edu::slims/dg4d3xq5bn . \n",
    "\n",
    "# folder and data should start transferring immediately\n",
    "\n",
    "# detach from screen: ctrl a + d\n",
    "\n",
    "# to see screens running\n",
    "screen -r\n",
    "\n",
    "# to end screen\n",
    "exit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move Data Around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv * /home/seuge91/fire/\n",
    "\n",
    "mv * /home/seuge91/fire/reads/raw3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anatomy of a Project Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bowtie\n",
    "    ref\n",
    "drep\n",
    "    all\n",
    "        all_dRep\n",
    "            data\n",
    "            data_tables\n",
    "            dereplicated_genomes \n",
    "            figures\n",
    "            log\n",
    "        split_contigs\n",
    "    drep.sh\n",
    "    vibrant_good\n",
    "megahit\n",
    "    err\n",
    "    log\n",
    "    contigs\n",
    "    renamed_contigs\n",
    "reads\n",
    "    err\n",
    "    log\n",
    "    raw\n",
    "    rmphix\n",
    "    rmphix_unpaired\n",
    "    stats\n",
    "    trimmed\n",
    "    unpaired\n",
    "sampleIDs.txt\n",
    "    # to create this\n",
    "    cd /home/seuge91/fire/reads/raw\n",
    "    ls -1 | cut -f1,2,3 -d_ | sort | uniq > sampleIDs.txt\n",
    "    # takes the first three elements of the sample name that are separated by the \"_\" delimitator\n",
    "    # remove extraneous labels like @md5Sum.md5\n",
    "    # make a txt file for each batch of raw reads (aka, sampleIDs2.txt, sampleIDs3.txt)\n",
    "unfinishedIDs.txt\n",
    "scripts\n",
    "    qual_filter.sh\n",
    "trimmed\n",
    "vibrant\n",
    "    err\n",
    "    log\n",
    "    # all the _vibrant directories for each sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quality Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nano qual_filter.sh\n",
    "\n",
    "#!/bin/bash\n",
    "#SBATCH -D /home/seuge91/fire/scripts/\n",
    "#SBATCH --job-name=qual_filter\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --time=04:00:00\n",
    "#SBATCH --ntasks=8\n",
    "#SBATCH --mem=32GB\n",
    "#SBATCH --partition=bmm\n",
    "\n",
    "# for calculating the amount of time the job takes\n",
    "begin=`date +%s`\n",
    "echo $HOSTNAME\n",
    "\n",
    "# loading modules\n",
    "module load bbmap\n",
    "source /home/csantosm/initconda\n",
    "conda activate TRIMMOMATIC\n",
    "\n",
    "# running commands\n",
    "path=${1}\n",
    "sample=${2}\n",
    "\n",
    "cd ${path}\n",
    "\n",
    "trimmomatic PE -threads 8 -phred33 \\\n",
    "  ./reads/raw/${sample}_R1_001.fq.gz ./reads/raw/${sample}_R2_001.fq.gz \\\n",
    "  ./reads/trimmed/${sample}_R1_trimmed.fq.gz ./reads/unpaired/${sample}_R1_unpaired.fq.gz \\\n",
    "  ./reads/trimmed/${sample}_R2_trimmed.fq.gz ./reads/unpaired/${sample}_R2_unpaired.fq.gz \\\n",
    "  ILLUMINACLIP:/home/csantosm/databases/TruSeq3-PE.fa:2:30:10 \\\n",
    "  SLIDINGWINDOW:4:30 MINLEN:50\n",
    "\n",
    "  bbduk.sh \\\n",
    "    in1=./reads/trimmed/${sample}_R1_trimmed.fq.gz \\\n",
    "    in2=./reads/trimmed/${sample}_R2_trimmed.fq.gz \\\n",
    "    out1=./reads/rmphix/${sample}_R1_rmphix.fq.gz \\\n",
    "    out2=./reads/rmphix/${sample}_R2_rmphix.fq.gz \\\n",
    "    ref=/home/csantosm/databases/phix174_ill.ref.fa \\\n",
    "    k=31 \\\n",
    "    hdist=1 \\\n",
    "    stats=./reads/stats/${sample}_stats.txt \\\n",
    "    -Xmx20g\n",
    "\n",
    "  bbduk.sh \\\n",
    "    in=./reads/unpaired/${sample}_R1_unpaired.fq.gz \\\n",
    "    out=./reads/rmphix_unpaired/${sample}_R1_rmphix_unpaired.fq.gz \\\n",
    "    ref=/home/csantosm/databases/phix174_ill.ref.fa \\\n",
    "    k=31 \\\n",
    "    hdist=1 \\\n",
    "    stats=./reads/stats/${sample}_R1_stats.txt \\\n",
    "    -Xmx20g\n",
    "\n",
    "  bbduk.sh \\\n",
    "    in=./reads/unpaired/${sample}_R2_unpaired.fq.gz \\\n",
    "    out=./reads/rmphix_unpaired/${sample}_R2_rmphix_unpaired.fq.gz \\\n",
    "    ref=/home/csantosm/databases/phix174_ill.ref.fa \\\n",
    "    k=31 \\\n",
    "    hdist=1 \\\n",
    "    stats=./reads/stats/${sample}_R2_stats.txt \\\n",
    "    -Xmx20g\n",
    "\n",
    "# finished commands\n",
    "\n",
    "# getting end time to calculate time elapsed\n",
    "end=`date +%s`\n",
    "elapsed=`expr $end - $begin`\n",
    "echo Time taken: $elapsed\n",
    "    \n",
    "----------------------------\n",
    "\n",
    "# to run commands (batch 1)\n",
    "\n",
    "bash\n",
    "for sample in $(</home/seuge91/fire/sampleIDs.txt)\n",
    "do\n",
    "sbatch \\\n",
    "--output=/home/seuge91/fire/reads/log/${sample}.qf.log \\\n",
    "--error=/home/seuge91/fire/reads/err/${sample}.qf.err \\\n",
    "/home/seuge91/fire/scripts/qual_filter.sh /home/seuge91/fire/ $sample\n",
    "done\n",
    "\n",
    "# to run commands (batch 2)\n",
    "\n",
    "bash\n",
    "for sample in $(</home/seuge91/fire/sampleIDs2.txt)\n",
    "do\n",
    "sbatch \\\n",
    "--output=/home/seuge91/fire/reads/log/${sample}.qf.log \\\n",
    "--error=/home/seuge91/fire/reads/err/${sample}.qf.err \\\n",
    "/home/seuge91/fire/scripts/qual_filter2.sh /home/seuge91/fire/ $sample\n",
    "done\n",
    "\n",
    "# to run commands (batch 3)\n",
    "\n",
    "bash\n",
    "for sample in $(</home/seuge91/fire/sampleIDs3.txt)\n",
    "do\n",
    "sbatch \\\n",
    "--output=/home/seuge91/fire/reads/log/${sample}.qf.log \\\n",
    "--error=/home/seuge91/fire/reads/err/${sample}.qf.err \\\n",
    "/home/seuge91/fire/scripts/qual_filter3.sh /home/seuge91/fire/ $sample\n",
    "done\n",
    "\n",
    "# to run commands (heat)\n",
    "\n",
    "bash\n",
    "for sample in $(</home/seuge91/heat/sampleIDs.txt)\n",
    "do\n",
    "sbatch \\\n",
    "--output=/home/seuge91/heat/reads/log/${sample}.qf.log \\\n",
    "--error=/home/seuge91/heat/reads/err/${sample}.qf.err \\\n",
    "/home/seuge91/heat/scripts/qual_filter.sh /home/seuge91/heat/ $sample\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenate files (if multiple batches!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create sample ID text file that only has the first part of the sample ID \n",
    "## that is consistent among all batches that you can use to concatenate files\n",
    "\n",
    "cd /home/seuge91/fire/reads/raw\n",
    "    ls -1 | cut -f1 -d_ | sort | uniq > sample.txt\n",
    "\n",
    "# for paired reads\n",
    "\n",
    "nano catpaired.sh\n",
    "\n",
    "----------------------------\n",
    "\n",
    "#!/bin/bash\n",
    "#SBATCH -D /home/seuge91/fire/reads/\n",
    "#SBATCH --job-name=catpaired\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH -t 8:00:00\n",
    "#SBATCH --ntasks=4\n",
    "#SBATCH --partition=bmm\n",
    "\n",
    "for file in $(</home/seuge91/fire/sample.txt)\n",
    "do\n",
    "cat rmphix/${file}_*_*_R1_*.fq.gz rmphix2/${file}_*_*_R1_*.fq.gz rmphix3/${file}_*_*_R1_*.fq.gz >> ${file}_R1_rmphix_combined.fq.gz\n",
    "cat rmphix/${file}_*_*_R2_*.fq.gz rmphix2/${file}_*_*_R2_*.fq.gz rmphix3/${file}_*_*_R2_*.fq.gz >> ${file}_R2_rmphix_combined.fq.gz\n",
    "done\n",
    "\n",
    "----------------------------\n",
    "\n",
    "# to run command\n",
    "\n",
    "sbatch catpaired.sh\n",
    "\n",
    "# for unpaired reads\n",
    "\n",
    "nano catunpaired.sh\n",
    "\n",
    "----------------------------\n",
    "\n",
    "#!/bin/bash\n",
    "#SBATCH -D /home/seuge91/fire/reads/\n",
    "#SBATCH --job-name=catunpaired\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH -t 8:00:00\n",
    "#SBATCH --ntasks=4\n",
    "#SBATCH --partition=bmm\n",
    "\n",
    "for file in $(</home/seuge91/fire/sample.txt)\n",
    "do\n",
    "cat rmphix_unpaired/${file}_*_*_R1_*.fq.gz rmphix_unpaired2/${file}_*_*_R1_*.fq.gz rmphix_unpaired3/${file}_*_*_R1_*.fq.gz >> ${file}_R1_rmphix_combined_unpaired.fq.gz\n",
    "cat rmphix_unpaired/${file}_*_*_R2_*.fq.gz rmphix_unpaired2/${file}_*_*_R2_*.fq.gz rmphix_unpaired3/${file}_*_*_R2_*.fq.gz >> ${file}_R2_rmphix_combined_unpaired.fq.gz\n",
    "done\n",
    "\n",
    "----------------------------\n",
    "\n",
    "# to run command\n",
    "\n",
    "sbatch catunpaired.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assembly\n",
    "\n",
    "## megahit\n",
    "\n",
    "progressive k-mer assembly, starts with k-mer size of 27, and then goes up by k-mer step of 10\n",
    "our settings starts k-mer at 27 and end at 87 (--presets meta-large)\n",
    "builds graph with k-mer 27 first, those are the first contigs\n",
    "then uses those contigs in addition to all the reads to build graph with k-mer 37\n",
    "small differences will \"explode the graph\" as k-mer size increases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "cd ~/fire/scripts\n",
    "nano megahit.sh\n",
    "\n",
    "----------------------------\n",
    "\n",
    "#!/bin/bash\n",
    "#SBATCH -D /home/seuge91/fire/scripts/\n",
    "#SBATCH --job-name=megahit\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH -t 24:00:00\n",
    "#SBATCH --ntasks=8\n",
    "#SBATCH --partition=bmh\n",
    "\n",
    "# for calculaticdng the amount of time the job takes\n",
    "begin=`date +%s`\n",
    "echo $HOSTNAME\n",
    "\n",
    "# loading modules\n",
    "source /home/csantosm/initconda\n",
    "conda activate MEGAHIT\n",
    "\n",
    "# move to rawreads folder\n",
    "cd /home/seuge91/fire/reads/\n",
    "sample=${1}\n",
    "\n",
    "megahit -1 ./rmphix_combined/${sample}_R1_rmphix_combined.fq.gz \\\n",
    "-2 ./rmphix_combined/${sample}_R2_rmphix_combined.fq.gz \\\n",
    "-r ./rmphix_combined_unpaired/${sample}_R1_rmphix_combined_unpaired.fq.gz,./rmphix_combined_unpaired/${sample}_R2_rmphix_combined_unpaired.fq.gz \\\n",
    "-o ../megahit/${sample} \\\n",
    "--out-prefix ${sample} \\\n",
    "--min-contig-len 10000 --presets meta-large \\\n",
    "-t 8 --continue\n",
    "\n",
    "# getting end time to calculate time elapsed\n",
    "end=`date +%s`\n",
    "elapsed=`expr $end - $begin`\n",
    "echo Time taken: $elapsed\n",
    "    \n",
    "----------------------------\n",
    "\n",
    "# to run commands\n",
    "\n",
    "bash\n",
    "for sample in $(</home/seuge91/fire/sample.txt)\n",
    "do\n",
    "sbatch \\\n",
    "--output=/home/seuge91/fire/megahit/log/${sample}.mh.log \\\n",
    "--error=/home/seuge91/fire/megahit/err/${sample}.mh.err \\\n",
    "/home/seuge91/fire/scripts/megahit.sh $sample\n",
    "done\n",
    "\n",
    "bash\n",
    "for sample in $(</home/seuge91/heat/sampleIDs.txt)\n",
    "do\n",
    "sbatch \\\n",
    "--output=/home/seuge91/heat/megahit/log/${sample}.mh.log \\\n",
    "--error=/home/seuge91/heat/megahit/err/${sample}.mh.err \\\n",
    "/home/seuge91/heat/scripts/megahit.sh $sample\n",
    "done\n",
    "\n",
    "___\n",
    "\n",
    "bash\n",
    "for sample in $(</home/seuge91/fire/newsample.txt)\n",
    "do\n",
    "sbatch \\\n",
    "--output=/home/seuge91/fire/megahit/log/${sample}.mh.log \\\n",
    "--error=/home/seuge91/fire/megahit/err/${sample}.mh.err \\\n",
    "/home/seuge91/fire/scripts/newmegahit.sh $sample\n",
    "done\n",
    "\n",
    "bash\n",
    "for sample in $(</home/seuge91/fire/unfinished2.txt)\n",
    "do\n",
    "sbatch \\\n",
    "--output=/home/seuge91/fire/megahit/log/${sample}.mh.log \\\n",
    "--error=/home/seuge91/fire/megahit/err/${sample}.mh.err \\\n",
    "/home/seuge91/fire/scripts/megahit.sh $sample\n",
    "done\n",
    "\n",
    "bash\n",
    "for sample in $(</home/seuge91/fire/unfinished3.txt)\n",
    "do\n",
    "sbatch \\\n",
    "--output=/home/seuge91/fire/megahit/log/${sample}.mh.log \\\n",
    "--error=/home/seuge91/fire/megahit/err/${sample}.mh.err \\\n",
    "/home/seuge91/fire/scripts/newmegahit.sh $sample\n",
    "done\n",
    "\n",
    "bash\n",
    "for sample in $(</home/seuge91/fire/unfinished4.txt)\n",
    "do\n",
    "sbatch \\\n",
    "--output=/home/seuge91/fire/megahit/log/${sample}.mh.log \\\n",
    "--error=/home/seuge91/fire/megahit/err/${sample}.mh.err \\\n",
    "/home/seuge91/fire/scripts/megahit.sh $sample\n",
    "done\n",
    "\n",
    "# to run samples that were suspended/unfinished\n",
    "# make updated sampleIDs list with samples that need to be resubmitted \n",
    "# I made unfinishedIDs.txt \n",
    "\n",
    "nano unfinishedIDs.txt\n",
    "\n",
    "# to run commands\n",
    "\n",
    "bash\n",
    "for sample in $(</home/seuge91/heat/unfinishedIDs.txt)\n",
    "do\n",
    "sbatch \\\n",
    "--output=/home/seuge91/heat/megahit/log/${sample}.mh.log \\\n",
    "--error=/home/seuge91/heat/megahit/err/${sample}.mh.err \\\n",
    "/home/seuge91/heat/scripts/megahit.sh $sample\n",
    "done\n",
    "\n",
    "----------------------------\n",
    "\n",
    "# to count how many contigs assembled in each sample\n",
    "\n",
    "grep -rc \"k127\" \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viral Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd megahit\n",
    "mkdir contigs\n",
    "mkdir renamed_contigs\n",
    "\n",
    "# move all contig fasta files into one folder\n",
    "cd /home/seuge91/fire/megahit\n",
    "mv */*contigs.fa contigs\n",
    "cd contigs\n",
    "\n",
    "# rename contig files so that it shows exactly what sample each contig came from\n",
    "module load bbmap\n",
    "\n",
    "for sample in $(<../../sample.txt)\n",
    "do\n",
    "rename.sh in=${sample}.contigs.fa out=${sample}.renamed.contigs.fa prefix=${sample}_contig_\n",
    "done\n",
    "\n",
    "for sample in $(<../../sampleIDs.txt)\n",
    "do\n",
    "rename.sh in=${sample}.contigs.fa out=${sample}.renamed.contigs.fa prefix=${sample}_contig_\n",
    "done\n",
    "\n",
    "# I accidentally deleted the contents of C10_S58_L004.renamed.contigs.fa - I tried running the below script but it does not work\n",
    "rename.sh in=C10_S58_L004.contigs.fa out=C10_S58_L004.renamed.contigs.fa prefix=C10_S58_L004_contig_\n",
    "\n",
    "# it didn't work because java wasnt installed\n",
    "# had to load deprecated version for it to work\n",
    "module load deprecated/java\n",
    "\n",
    "\n",
    "\n",
    "mv *renamed.contigs.fa ../renamed_contigs\n",
    "\n",
    "cd /home/seuge91/fire\n",
    "mkdir vibrant\n",
    "cd vibrant\n",
    "mkdir err log\n",
    "\n",
    "# make vibrant script\n",
    "nano vibrant.sh\n",
    "\n",
    "#below is an old script (keep scrolling for most updated one)\n",
    "----------------------------\n",
    "\n",
    "#!/bin/bash\n",
    "#SBATCH -D /home/seuge91/fire/scripts/\n",
    "#SBATCH --job-name=vbrnt\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH -t 12:00:00\n",
    "#SBATCH --ntasks=4\n",
    "#SBATCH --partition=bmh\n",
    "\n",
    "# for calculating the amount of time the job takes\n",
    "begin=`date +%s`\n",
    "echo $HOSTNAME\n",
    "\n",
    "#activate personal conda env\n",
    "source /home/csantosm/initconda\n",
    "conda activate VIBRANT\n",
    "\n",
    "# move to main folder\n",
    "cd /home/seuge91/fire/\n",
    "sample=${1}\n",
    "\n",
    "VIBRANT_run.py -i ./megahit/renamed_contigs/${sample}.renamed.contigs.fa \\\n",
    "-folder ./vibrant/${sample}_vibrant \\\n",
    "-t 4 -f nucl -virome\n",
    "\n",
    "#getting end time to calculate time elapsed\n",
    "end=`date +%s`\n",
    "elapsed=`expr $end - $begin`\n",
    "echo Time taken: $elapsed\n",
    "   \n",
    "----------------------------\n",
    "#below is an old script (keep scrolling for most updated one)\n",
    "----------------------------\n",
    "\n",
    "#!/bin/bash\n",
    "#SBATCH -D /home/seuge91/heat/scripts/\n",
    "#SBATCH --job-name=vbrnt\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH -t 12:00:00\n",
    "#SBATCH --ntasks=4\n",
    "#SBATCH --partition=bmh\n",
    "\n",
    "# for calculating the amount of time the job takes\n",
    "begin=`date +%s`\n",
    "echo $HOSTNAME\n",
    "\n",
    "#activate personal conda env\n",
    "source /home/csantosm/initconda\n",
    "conda activate VIBRANT\n",
    "\n",
    "# move to main folder\n",
    "cd /home/seuge91/heat/\n",
    "sample=${1}\n",
    "\n",
    "VIBRANT_run.py -i ./megahit/renamed_contigs/${sample}.renamed.contigs.fa \\\n",
    "-folder ./vibrant/${sample}_vibrant \\\n",
    "-t 4 -f nucl -virome\n",
    "\n",
    "#getting end time to calculate time elapsed\n",
    "end=`date +%s`\n",
    "elapsed=`expr $end - $begin`\n",
    "echo Time taken: $elapsed\n",
    "    \n",
    "----------------------------\n",
    "\n",
    "# for some reason these files were incomplete, I'm going to try running again with original script\n",
    "\n",
    "###new script 8/15/2023 (did not work, a bunch of temp files???)\n",
    "\n",
    "mkdir vibrant_sara\n",
    "cd vibrant_sara\n",
    "mkdir err log\n",
    "----------------------------\n",
    "\n",
    "#!/bin/bash\n",
    "#SBATCH -D /home/seuge91/heat/scripts/\n",
    "#SBATCH --job-name=vbrnt\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH -t 12:00:00\n",
    "#SBATCH --ntasks=4\n",
    "#SBATCH --partition=bmh\n",
    "\n",
    "# for calculating the amount of time the job takes\n",
    "begin=`date +%s`\n",
    "echo $HOSTNAME\n",
    "\n",
    "#activate personal conda env\n",
    "source /home/csantosm/initconda\n",
    "conda activate vibrant_sara\n",
    "\n",
    "# move to main folder\n",
    "cd /home/seuge91/heat/\n",
    "sample=${1}\n",
    "\n",
    "~/VIBRANT/VIBRANT/VIBRANT_run.py -i ./megahit/renamed_contigs/${sample}.renamed.contigs.fa \\\n",
    "-folder ./vibrant_sara/${sample}_vibrant \\\n",
    "-t 4 -f nucl -virome\n",
    "\n",
    "#getting end time to calculate time elapsed\n",
    "end=`date +%s`\n",
    "elapsed=`expr $end - $begin`\n",
    "echo Time taken: $elapsed\n",
    "    \n",
    "----------------------------\n",
    "\n",
    "# to run commands\n",
    "\n",
    "bash\n",
    "for sample in $(</home/seuge91/heat/sampleIDs.txt)\n",
    "do\n",
    "sbatch \\\n",
    "--output=/home/seuge91/heat/vibrant_sara/log/${sample}.vb.log \\\n",
    "--error=/home/seuge91/heat/vibrant_sara/err/${sample}.vb.err \\\n",
    "/home/seuge91/heat/scripts/vibrant_sara.sh $sample\n",
    "done\n",
    "\n",
    "# to run commands on suspended jobs (make sure to edit unfinishedIDs.txt)\n",
    "\n",
    "bash\n",
    "for sample in $(</home/seuge91/fire/unfinishedIDs.txt)\n",
    "do\n",
    "sbatch \\\n",
    "--output=/home/seuge91/fire/vibrant/log/${sample}.vb.log \\\n",
    "--error=/home/seuge91/fire/vibrant/err/${sample}.vb.err \\\n",
    "/home/seuge91/fire/scripts/vibrant.sh $sample\n",
    "done\n",
    "\n",
    "\n",
    "\n",
    "# redo script:\n",
    "\n",
    "----------------------------\n",
    "\n",
    "#!/bin/bash\n",
    "#SBATCH -D /home/seuge91/heat/scripts/\n",
    "#SBATCH --job-name=vbrnt\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH -t 12:00:00\n",
    "#SBATCH --ntasks=4\n",
    "#SBATCH --partition=bmh\n",
    "\n",
    "# for calculating the amount of time the job takes\n",
    "begin=`date +%s`\n",
    "echo $HOSTNAME\n",
    "\n",
    "#activate personal conda env\n",
    "source /home/csantosm/initconda\n",
    "conda activate VIBRANT\n",
    "\n",
    "# move to main folder\n",
    "cd /home/seuge91/heat/\n",
    "sample=${1}\n",
    "\n",
    "VIBRANT_run.py -i ./megahit/renamed_contigs/${sample}.renamed.contigs.fa \\\n",
    "-folder ./vibrant_redo/${sample}_vibrant \\\n",
    "-t 4 -f nucl -virome\n",
    "\n",
    "#getting end time to calculate time elapsed\n",
    "end=`date +%s`\n",
    "elapsed=`expr $end - $begin`\n",
    "echo Time taken: $elapsed\n",
    "   \n",
    "----------------------------\n",
    "\n",
    "# to run commands on jobs to redo\n",
    "\n",
    "bash\n",
    "for sample in $(</home/seuge91/heat/redoIDs.txt)\n",
    "do\n",
    "sbatch \\\n",
    "--output=/home/seuge91/heat/vibrant_redo/log/${sample}.vb.log \\\n",
    "--error=/home/seuge91/heat/vibrant_redo/err/${sample}.vb.err \\\n",
    "/home/seuge91/heat/scripts/vibrant.sh $sample\n",
    "done\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of phages in each sample (if samples were not renamed)\n",
    "\n",
    "grep -rc \"k87\" # but this gives you a number for everything\n",
    "\n",
    "grep -rc \"k87\" --include=\"*phages_combined.txt\" # this specifies the filename to look in\n",
    "\n",
    "# count number of phages in each sample (if samples were renamed)\n",
    "\n",
    "grep -rc \"contig\" --include=\"*phages_combined.txt\"\n",
    "\n",
    "# count number of circular, high, and medium quality drafts in each sample\n",
    "grep -rc 'circular\\|high\\|medium' --include=\"*genome_quality*\"\n",
    "\n",
    "grep -rc 'circular' --include=\"*genome_quality*\"\n",
    "\n",
    "# to separate output into fields for easy table export\n",
    "cd /home/seuge91/heat/vibrant\n",
    "grep -rc 'low' --include=\"*genome_quality*\" | awk -F[:/] '{print $1,$5}' > lowcontigs.txt\n",
    "grep -rc 'medium' --include=\"*genome_quality*\" | awk -F[:/] '{print $1,$5}' > mediumcontigs.txt\n",
    "grep -rc 'high' --include=\"*genome_quality*\" | awk -F[:/] '{print $1,$5}' > highcontigs.txt\n",
    "grep -rc 'circular' --include=\"*genome_quality*\" | awk -F[:/] '{print $1,$5}' > circularcontigs.txt\n",
    "\n",
    "grep -rc \"contig\" --include=\"*phages_combined.fna\" | awk -F[:] '{print $1,$2}' > ../viralcontigs.txt\n",
    "\n",
    "#to download these files\n",
    "rsync -r --progress seuge91@farm:~/heat/vibrant/*contigs.txt /mnt/c/Users/segeo/Downloads\n",
    "rsync -r --progress seuge91@farm:~/heat/vibrant/viralcontigs.txt /mnt/c/Users/segeo/Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloading files (you need to be signed out)\n",
    "\n",
    "# download files of each sample into appropriate folder on box\n",
    "\n",
    "rsync -r --progress seuge91@farm:~/heat/vibrant/*_vibrant/*renamed.contigs/VIBRANT_figures_* /mnt/c/Users/segeo/Downloads\n",
    "    \n",
    "rsync -r --progress seuge91@farm:~/blodgett/Project_JESG_Nova348P_Geonczy/vibrant/space.txt /mnt/c/Users/segeo/Downloads\n",
    "\n",
    "rsync -r --progress seuge91@farm:~/heat/vibrant/*contigs.txt /mnt/c/Users/segeo/Downloads\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dereplication\n",
    "## https://drep.readthedocs.io/en/latest/overview.html\n",
    "https://drep.readthedocs.io/en/latest/module_descriptions.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned-up! (this is for heat study)\n",
    "\n",
    "# prepare files for dereplication\n",
    "# in vibrant folder, make new directory for vibrant contigs\n",
    "mkdir updated_vibrant_contigs\n",
    "\n",
    "# copy all contigs to directory\n",
    "cp */*/VIBRANT_phages*/*phages_combined.fna ./vibrant_contigs\n",
    "\n",
    "# in project directory, make drep directory\n",
    "mkdir updated_drep\n",
    "\n",
    "# make subdirectory\n",
    "mkdir all\n",
    "\n",
    "# make folder for split contigs in each subdirectory\n",
    "mkdir split_contigs\n",
    "\n",
    "# move all viral contigs from each sample from vibrant to the folder called \"split_contigs\" ...\n",
    "# ...by first moving\" into vibrant folder and then vibrant_contigs directory\n",
    "cd vibrant\n",
    "cd updated_vibrant_contigs\n",
    "\n",
    "# concatenate all viral contigs into a single file\n",
    "cat *phages_combined.fna > all.vib.contigs.fna\n",
    "\n",
    "# count how many total contigs\n",
    "grep -c contig all.vib.contigs.fna\n",
    "\n",
    "# first cluster using CD HIT\n",
    "# rename file to use for CD Hit (input.fna)\n",
    "cp all.vib.contigs.fna ./input.fna\n",
    "\n",
    "# create CD HIT script\n",
    "nano cdhit.sh\n",
    "\n",
    "-----------------\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=cdhit\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH -t 24:00:00\n",
    "#SBATCH --ntasks=10\n",
    "#SBATCH --output=cdhit%j.out\n",
    "#SBATCH --error=cdhit%j.err\n",
    "#SBATCH --partition=bmh\n",
    "\n",
    "module load cdhit\n",
    "cd-hit-est -i input.fna -o clustered_heat.fna \\\n",
    "-c 0.95 -aS 0.85 -M 7000 -T 10\n",
    "-----------------\n",
    "\n",
    "# run CD HIT script\n",
    "sbatch cdhit.sh\n",
    "\n",
    "# number of contigs after CD HIT: 141,638 --> 34,948\n",
    "\n",
    "# now run dereplication\n",
    "\n",
    "# take the concatenated file and move it to the appropriate split_contig folder within a drep/\"subset\"/split_contigs\n",
    "mv clustered_heat.fna cd /home/seuge91/heat/updated_drep/all/split_contigs\n",
    "\n",
    "# this works even if there is some weird error that shows up\n",
    "\n",
    "# separate the concatenated fasta file into multiple ones\n",
    "# use screen if you have a lot of contigs (this will take awhile)\n",
    "awk '/^>/ {OUT=substr($0,2) \".fa\"}; OUT {print >OUT}' *.fna \n",
    "\n",
    "    # if using screen...you can monitor how close it is to done by counting file names\n",
    "        ls -1 | wc -l\n",
    "\n",
    "# once done, remove the concatenated file from folder (you can move up a folder)\n",
    "mv clustered_heat.fna ../\n",
    "\n",
    "# make updated_drep script\n",
    "nano updated_drep.sh\n",
    "-----------------------------------------------------------------------------------------\n",
    "\n",
    "#!/bin/bash\n",
    "#SBATCH -D /home/seuge91/heat/updated_drep\n",
    "#SBATCH --job-name=drep\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH -t 72:00:00\n",
    "#SBATCH --ntasks=24\n",
    "#SBATCH --mem=900GB\n",
    "#SBATCH --partition=bmh\n",
    "\n",
    "# for calculating the amount of time the job takes\n",
    "begin=`date +%s`\n",
    "echo $HOSTNAME\n",
    "\n",
    "# activate personal conda env\n",
    "source /home/csantosm/initconda\n",
    "conda activate DREP\n",
    "\n",
    "treatment=${1}\n",
    "\n",
    "cd /home/seuge91/heat/updated_drep/${treatment}\n",
    "\n",
    "dRep dereplicate ./${treatment}_dRep \\\n",
    "-g ./split_contigs/*.fa \\\n",
    "--S_algorithm ANImf \\\n",
    "-sa 0.95 \\\n",
    "-nc 0.85 \\\n",
    "-l 10000 \\\n",
    "-N50W 0 \\\n",
    "-sizeW 1 \\\n",
    "--ignoreGenomeQuality \\\n",
    "--clusterAlg single \\\n",
    "-p 24\n",
    "\n",
    "#getting end time to calculate time elapsed\n",
    "end=`date +%s`\n",
    "elapsed=`expr $end - $begin`\n",
    "echo Time taken: $elapsed\n",
    "    \n",
    "-----------------------------------------------------------------------------------------\n",
    "\n",
    "# run the script giving the variable of the subset folder that has the split_contigs directory\n",
    "sbatch updated_drep.sh all\n",
    "\n",
    "-----------------------------------------------------------------------------------------\n",
    "\n",
    "# number of contigs after drep: 34,948 --> 18,869\n",
    "\n",
    "# to zip all.drep.contigs.fa file and download so that database of dereplicated vOTUs can be uploaded to zenodo\n",
    "gzip -k all.drep.contigs.fa\n",
    "scp seuge91@farm.cse.ucdavis.edu:/home/seuge91/heat/updated_bowtie/all.drep.contigs.fa.gz /mnt/c/Users/segeo/Downloads\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell is messy so refer to above cell /\\\n",
    "\n",
    "# prepare files for dereplication\n",
    "# in vibrant folder, make new directory for vibrant contigs\n",
    "mkdir vibrant_contigs\n",
    "\n",
    "# copy all contigs to directory\n",
    "cp */*/VIBRANT_phages*/*phages_combined.fna ./vibrant_contigs\n",
    "\n",
    "# in project directory, make drep directory\n",
    "mkdir drep\n",
    "\n",
    "# make subdirectory\n",
    "mkdir all\n",
    "\n",
    "# make folder for split contigs in each subdirectory\n",
    "mkdir split_contigs\n",
    "\n",
    "# move all viral contigs from each sample from vibrant to the folder called \"split_contigs\" ...\n",
    "# ...by first moving\" into vibrant folder and then vibrant_contigs directory\n",
    "cd vibrant\n",
    "cd vibrant_contigs\n",
    "\n",
    "# concatenate all viral contigs into a single file\n",
    "cat *phages_combined.fna > all.vib.contigs.fna\n",
    "\n",
    "# count how many total contigs\n",
    "grep -c contig all.vib.contigs.fna\n",
    "\n",
    "    # fire data - ~115,000, maybe too much?\n",
    "    # updated fire data - 404,439\n",
    "    # IF THIS IS A LOT MORE THAN 100,000 YOU SHOULD ONLY TAKE MEDIUM AND HIGH QUALITY CONTIGS\n",
    "    # heat data = 139,398 contigs\n",
    "\n",
    "# take the concatenated file and move it to the appropriate split_contig folder within a drep/\"subset\"/split_contigs\n",
    "mv all.vib.contigs.fna cd /home/seuge91/fire/drep/all/split_contigs\n",
    "mv all.vib.contigs.fna cd /home/seuge91/heat/drep/all/split_contigs\n",
    "\n",
    "# this works even if there is some weird error that shows up\n",
    "\n",
    "# separate the concatenated fasta file into multiple ones\n",
    "awk '/^>/ {OUT=substr($0,2) \".fa\"}; OUT {print >OUT}' *.fna \n",
    "\n",
    "    #is this supposed to take a long time? yes, if you have a lot of contigs...use screen\n",
    "    # you can monitor how close it is to done by counting file names\n",
    "        ls -1 | wc -l\n",
    "\n",
    "# once done, remove the concatenated file\n",
    "rm all.vib.contigs.fna\n",
    "\n",
    "-----------------------------------------------------------------------------------------\n",
    "\n",
    "#!/bin/bash\n",
    "#SBATCH -D /home/seuge91/fire/drep\n",
    "#SBATCH --job-name=drep\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH -t 72:00:00\n",
    "#SBATCH --ntasks=24\n",
    "#SBATCH --mem=900GB\n",
    "#SBATCH --partition=bmh\n",
    "\n",
    "# for calculating the amount of time the job takes\n",
    "begin=`date +%s`\n",
    "echo $HOSTNAME\n",
    "\n",
    "# activate personal conda env\n",
    "source /home/csantosm/initconda\n",
    "conda activate DREP\n",
    "\n",
    "treatment=${1}\n",
    "\n",
    "cd /home/seuge91/fire/drep/${treatment}\n",
    "\n",
    "dRep dereplicate ./${treatment}_dRep \\\n",
    "-g ./split_contigs/*.fa \\\n",
    "--S_algorithm ANImf \\\n",
    "-sa 0.95 \\\n",
    "-nc 0.85 \\\n",
    "-l 10000 \\\n",
    "-N50W 0 \\\n",
    "-sizeW 1 \\\n",
    "--ignoreGenomeQuality \\\n",
    "--clusterAlg single \\\n",
    "-p 24\n",
    "\n",
    "#getting end time to calculate time elapsed\n",
    "end=`date +%s`\n",
    "elapsed=`expr $end - $begin`\n",
    "echo Time taken: $elapsed\n",
    "    \n",
    "-----------------------------------------------------------------------------------------\n",
    "\n",
    "# run the script giving the variable of the subset folder that has the split_contigs directory\n",
    "sbatch drep.sh all\n",
    "\n",
    "-----------------------------------------------------------------------------------------\n",
    "\n",
    "# may run out of memory, check slurm output files in drep directory\n",
    "\n",
    "# if this is the case, just use subset of medium and high quality genome drafts\n",
    "cat vibrant_contigs/*phages_combined.fna > all.vib.contigs.fna\n",
    "\n",
    "# to see how many contigs in all.vib.contigs.fna\n",
    "grep '>' all.vib.contigs.fna | wc -l\n",
    "\n",
    "# to get list of contigs that are medium and high quality\n",
    "grep 'circular\\|high\\|medium' */*/VIBRANT_results*/VIBRANT_genome_quality_* | cut -f1 | cut -f2 -d: | sort | uniq > good.vib.ids\n",
    "\n",
    "# to see how many contigs in file\n",
    "wc -l good.vib.ids\n",
    "\n",
    "    # fire - 107,053\n",
    "    \n",
    "# load Chris's conda space\n",
    "source /home/csantosm/initconda\n",
    "conda activate BIOPYTHON\n",
    "\n",
    "# run python script to get good contigs into one file\n",
    "python ../scripts/filter_fasta_by_list_of_headers.py all.vib.contigs.fna good.vib.ids > good.vib.contigs.fna\n",
    "\n",
    "    # actually the pathway changed to this??\n",
    "    /home/csantosm/general_scripts/filter_fasta_by_list_of_headers.py\n",
    "    # so try this?\n",
    "    python /home/csantosm/general_scripts/filter_fasta_by_list_of_headers.py all.vib.contigs.fna good.vib.ids > good.vib.contigs.fna\n",
    "\n",
    "    # 619 of the headers from list were not identified in the input fasta file\n",
    "    # yeah there might have been some name changes, but whatever\n",
    "\n",
    "# make another directory in drep folder for good contigs and move file to it\n",
    "mkdir vibrant_good\n",
    "mv ../vibrant/good.vib.contigs.fna vibrant_good\n",
    "\n",
    "# separate the concatenated fasta file into multiple ones\n",
    "# this can take awhile and you might get disconnected before it finishes, so use screen\n",
    "awk '/^>/ {OUT=substr($0,2) \".fa\"}; OUT {print >OUT}' *.fna \n",
    "\n",
    "# you can monitor how close it is to done by counting file names\n",
    "ls -1 | wc -l\n",
    "\n",
    "\n",
    "# make directory for split contigs within vibrant_good and move split contigs into this directory\n",
    "mkdir split_contigs\n",
    "mv *.fa split_contigs\n",
    "\n",
    "# remove the concatenated file\n",
    "rm good.vib.contigs.fna\n",
    "\n",
    "# run script again, but with new folder\n",
    "sbatch drep.sh vibrant_good\n",
    "\n",
    "-----------------------------------------------------------------------------------------\n",
    "\n",
    "# if this still doesn't work, split into smaller groups\n",
    "cat vibrant_contigs/B* > blodgett.vib.contigs.fna\n",
    "\n",
    "# to see how many contigs in file\n",
    "grep '>' blodgett.vib.contigs.fna | wc -l\n",
    "# 151,158 total just from B samples\n",
    "\n",
    "cat vibrant_contigs/U* >> blodgett.vib.contigs.fna\n",
    "\n",
    "# to make sure more were added\n",
    "grep '>' blodgett.vib.contigs.fna | wc -l\n",
    "# 327,006 total now, so yes, it was appended\n",
    "\n",
    "# just get medium/high/circular quality drafts from blodgett subset\n",
    "# load Chris's conda space\n",
    "source /home/csantosm/initconda\n",
    "conda activate BIOPYTHON\n",
    "# run python script to get good contigs into one file\n",
    "python /home/csantosm/general_scripts/filter_fasta_by_list_of_headers.py blodgett.vib.contigs.fna good.vib.ids > good.blodgett.vib.contigs.fna\n",
    "\n",
    "# count how many good contigs\n",
    "grep '>' good.blodgett.vib.contigs.fna | wc -l\n",
    "# 91,043 (91,043/151,158 = ~60%)\n",
    "\n",
    "# make another directory in drep folder for good contigs and move file to it\n",
    "mkdir vibrant_good_blodgett\n",
    "mv ../vibrant/good.blodgett.vib.contigs.fna vibrant_good_blodgett\n",
    "\n",
    "# separate the concatenated fasta file into multiple ones\n",
    "# this can take awhile and you might get disconnected before it finishes, so use screen\n",
    "awk '/^>/ {OUT=substr($0,2) \".fa\"}; OUT {print >OUT}' *.fna \n",
    "\n",
    "# you can monitor how close it is to done by counting file names\n",
    "ls -1 | wc -l\n",
    "\n",
    "\n",
    "# make directory for split contigs within vibrant_good and move split contigs into this directory\n",
    "mkdir split_contigs\n",
    "mv *.fa split_contigs\n",
    "\n",
    "# remove the concatenated file\n",
    "rm good.blodgett.vib.contigs.fna\n",
    "\n",
    "# run script again, but with new folder\n",
    "sbatch drep.sh vibrant_good_blodgett\n",
    "# this got killed because OUT OF MEMORY\n",
    "\n",
    "-----------------------------------------------------------------------------------------\n",
    "\n",
    "# if this still doesn't work, split into even smaller groups\n",
    "cat vibrant_contigs/B* > burn.vib.contigs.fna\n",
    "\n",
    "# to see how many contigs in file\n",
    "grep '>' burn.vib.contigs.fna | wc -l\n",
    "# 151,158 total just from B samples\n",
    "\n",
    "# split more\n",
    "cat vibrant_contigs/BS* > burn.surface.vib.contigs.fna\n",
    "cat vibrant_contigs/BD* > burn.deep.vib.contigs.fna\n",
    "\n",
    "grep '>' burn.surface.vib.contigs.fna | wc -l\n",
    "# 73,892 total from BS samples\n",
    "grep '>' burn.deep.vib.contigs.fna | wc -l\n",
    "# 77,266 total from BD samples\n",
    "\n",
    "cat vibrant_contigs/U* > control.vib.contigs.fna\n",
    "\n",
    "# to see how many contigs in file\n",
    "grep '>' control.vib.contigs.fna | wc -l\n",
    "# 175,848 total just from U samples\n",
    "\n",
    "grep '>' control.surface.vib.contigs.fna | wc -l\n",
    "# 90,491 total from US samples\n",
    "grep '>' control.deep.vib.contigs.fna | wc -l\n",
    "# 85,357 total from UD samples\n",
    "\n",
    "# just get medium/high/circular quality drafts from blodgett subset\n",
    "# load Chris's conda space\n",
    "source /home/csantosm/initconda\n",
    "conda activate BIOPYTHON\n",
    "# run python script to get good contigs into one file\n",
    "python /home/csantosm/general_scripts/filter_fasta_by_list_of_headers.py burn.vib.contigs.fna good.vib.ids > good.burn.vib.contigs.fna\n",
    "python /home/csantosm/general_scripts/filter_fasta_by_list_of_headers.py control.vib.contigs.fna good.vib.ids > good.control.vib.contigs.fna\n",
    "python /home/csantosm/general_scripts/filter_fasta_by_list_of_headers.py control.surface.vib.contigs.fna good.vib.ids > good.control.surface.vib.contigs.fna\n",
    "python /home/csantosm/general_scripts/filter_fasta_by_list_of_headers.py control.deep.vib.contigs.fna good.vib.ids > good.control.deep.vib.contigs.fna\n",
    "\n",
    "python /home/csantosm/general_scripts/filter_fasta_by_list_of_headers.py burn.surface.vib.contigs.fna good.vib.ids > good.burn.surface.vib.contigs.fna\n",
    "\n",
    "python /home/csantosm/general_scripts/filter_fasta_by_list_of_headers.py burn.deep.vib.contigs.fna good.vib.ids > good.burn.deep.vib.contigs.fna\n",
    "\n",
    "# count how many good contigs\n",
    "grep '>' good.burn.vib.contigs.fna | wc -l\n",
    "# 42,171 (42,171/151,158 = ~28%)\n",
    "grep '>' good.burn.surface.vib.contigs.fna | wc -l\n",
    "# 21,119 (21,119/73,892 = ~29%)\n",
    "grep '>' good.burn.deep.vib.contigs.fna | wc -l\n",
    "# 21,052 (21,052/77,266 = ~28%)\n",
    "grep '>' good.control.vib.contigs.fna | wc -l\n",
    "# 48,872 (48,872/175,848 = ~28%)\n",
    "grep '>' good.control.surface.vib.contigs.fna | wc -l\n",
    "# 26,221 (26,221/90,491 = ~29%)\n",
    "grep '>' good.control.deep.vib.contigs.fna | wc -l\n",
    "# 22,651 (22,651/85,357 = ~27%)\n",
    "\n",
    "# make another directory in drep folder for good contigs and move file to it\n",
    "mkdir vibrant_good_burn\n",
    "mv ../vibrant/good.burn.vib.contigs.fna vibrant_good_burn\n",
    "mkdir vibrant_good_burn_surface\n",
    "mv ../vibrant/good.burn.surface.vib.contigs.fna vibrant_good_burn_surface\n",
    "mkdir vibrant_good_burn_deep\n",
    "mv ../vibrant/good.burn.deep.vib.contigs.fna vibrant_good_burn_deep\n",
    "\n",
    "\n",
    "mkdir vibrant_good_control\n",
    "mv ../vibrant/good.control.vib.contigs.fna vibrant_good_control\n",
    "mkdir vibrant_good_control_surface\n",
    "mv ../vibrant/good.control.surface.vib.contigs.fna vibrant_good_control_surface\n",
    "mkdir vibrant_good_control_deep\n",
    "mv ../vibrant/good.control.deep.vib.contigs.fna vibrant_good_control_deep\n",
    "\n",
    "# separate the concatenated fasta file into multiple ones\n",
    "# this can take awhile and you might get disconnected before it finishes, so use screen\n",
    "screen\n",
    "awk '/^>/ {OUT=substr($0,2) \".fa\"}; OUT {print >OUT}' *.fna \n",
    "\n",
    "# you can monitor how close it is to done by counting file names\n",
    "ls -1 | wc -l\n",
    "\n",
    "\n",
    "# make directory for split contigs within vibrant_good and move split contigs into this directory\n",
    "mkdir split_contigs\n",
    "mv *.fa split_contigs\n",
    "\n",
    "# remove the concatenated file\n",
    "rm good.burn.vib.contigs.fna\n",
    "rm good.control.vib.contigs.fna\n",
    "\n",
    "# run script again, but with new folder\n",
    "sbatch drep.sh vibrant_good_burn\n",
    "# this one got cancelled due to time limit\n",
    "\n",
    "sbatch drep.sh vibrant_good_control\n",
    "# this one got fucked up, I don't know why\n",
    "\n",
    "sbatch drep.sh vibrant_good_control_deep\n",
    "# this one worked\n",
    "\n",
    "sbatch drep.sh vibrant_good_control_surface\n",
    "# this one worked\n",
    "\n",
    "-----------------------------------------------------------------------------------------\n",
    "\n",
    "cat vibrant_contigs/M* > lnu.vib.contigs.fna\n",
    "\n",
    "# to see how many contigs in file\n",
    "grep '>' lnu.vib.contigs.fna | wc -l\n",
    "# 40,003 total just from M samples\n",
    "\n",
    "cat vibrant_contigs/Q* >> lnu.vib.contigs.fna\n",
    "# to make sure more were added\n",
    "grep '>' lnu.vib.contigs.fna | wc -l\n",
    "# 77,433 total now, so yes, it was appended\n",
    "\n",
    "# just get medium/high/circular quality drafts from LNU subset\n",
    "# load Chris's conda space\n",
    "source /home/csantosm/initconda\n",
    "conda activate BIOPYTHON\n",
    "# run python script to get good contigs into one file\n",
    "python /home/csantosm/general_scripts/filter_fasta_by_list_of_headers.py lnu.vib.contigs.fna good.vib.ids > good.lnu.vib.contigs.fna\n",
    "\n",
    "python /home/csantosm/general_scripts/filter_fasta_by_list_of_headers.py c.vib.contigs.fna good.vib.ids > good.c.vib.contigs.fna\n",
    "\n",
    "python /home/csantosm/general_scripts/filter_fasta_by_list_of_headers.py hn.vib.contigs.fna good.vib.ids > good.hn.vib.contigs.fna\n",
    "\n",
    "python /home/csantosm/general_scripts/filter_fasta_by_list_of_headers.py hd.vib.contigs.fna good.vib.ids > good.hd.vib.contigs.fna\n",
    "\n",
    "# count how many good contigs\n",
    "grep '>' good.lnu.vib.contigs.fna | wc -l\n",
    "# 15,391 (15,391/77,433 = ~20%)\n",
    "\n",
    "grep '>' good.c.vib.contigs.fna | wc -l\n",
    "#9,318\n",
    "\n",
    "grep '>' good.hn.vib.contigs.fna | wc -l\n",
    "#8,911\n",
    "\n",
    "grep '>' good.hd.vib.contigs.fna | wc -l\n",
    "#18,786\n",
    "\n",
    "# make another directory in drep folder for good contigs and move file to it\n",
    "mkdir vibrant_good_lnu\n",
    "mv ../vibrant/good.lnu.vib.contigs.fna vibrant_good_lnu\n",
    "\n",
    "# separate the concatenated fasta file into multiple ones\n",
    "# this can take awhile and you might get disconnected before it finishes, so use screen\n",
    "awk '/^>/ {OUT=substr($0,2) \".fa\"}; OUT {print >OUT}' *.fna \n",
    "\n",
    "# you can monitor how close it is to done by counting file names\n",
    "ls -1 | wc -l\n",
    "\n",
    "# make directory for split contigs within vibrant_good and move split contigs into this directory\n",
    "mkdir split_contigs\n",
    "mv *.fa split_contigs\n",
    "\n",
    "# run script again, but with new folder\n",
    "sbatch drep.sh vibrant_good_lnu\n",
    "\n",
    "-----------------------------------------------------------------------------------------\n",
    "\n",
    "# when done:\n",
    "# move into dereplicated_genomes folder and calculate number of vOTUs\n",
    "# this is the total number of dereplicated vOTUs\n",
    "cd dereplicated_genomes\n",
    "ls -1 | wc -l\n",
    "\n",
    "# lnu - 13,709 dereplicated genomes\n",
    "# blodgett burn surface plots - 17,083\n",
    "# blodgett burn deep plots - 17,688\n",
    "    # blodgett burn total - 27,763\n",
    "# blodgett control surface plots - 21,939\n",
    "# blodgett control deep plots - 19,312\n",
    "    # blodgett control total - 34,706\n",
    "    \n",
    "# DEREPLICATION BURN + CONTROL (input of 62,469) FAILED (unstacked dataframe is too big, causing int32 overflow)\n",
    "# all contigs from these two groups are in fire/drep/blodgett/split_contigs\n",
    "    \n",
    "# heat dereplicated genomes\n",
    "# control - 2,752\n",
    "# dnase - 3,995\n",
    "# no dnase - 2,065\n",
    "# all good - 8,812 --> 4,644\n",
    "\n",
    "\n",
    "# move  into split_contigs folders to find original number of vOTUs\n",
    "cd split_contigs\n",
    "ls -1 | wc -l\n",
    "\n",
    "# YOU HAVE TO COMBINE SUB GROUPS INTO BIGGER GROUPS AND RUN DEREPLICATION AGAIN \n",
    "# UNTIL YOU HAVE COMPARED ALL DEREPLICATED GENOMES TO EACH OTHER\n",
    "\n",
    "cat burn/burn_dRep/dereplicated_genomes/*.fa > blodgett/blodgett.vib.contigs.fna\n",
    "cat control/control/_dRep/dereplicated_genomes/*.fa >> blodgett/blodgett.vib.contigs.fna\n",
    "\n",
    "cat control/control_dRep/dereplicated_genomes/*.fa > allgood/allgood.vib.contigs.fna\n",
    "cat dnase/dnase_dRep/dereplicated_genomes/*.fa >> allgood/allgood.vib.contigs.fna\n",
    "cat nodnase/nodnase_dRep/dereplicated_genomes/*.fa >> allgood/allgood.vib.contigs.fna\n",
    "\n",
    "# downloading csv tables (you need to be signed out)\n",
    "\n",
    "rsync -r --progress seuge91@farm:~/blodgett/Project_JESG_Nova348P_Geonczy/drep/all/all_dRep/data_tables/genomeInformation.csv /mnt/c/Users/segeo/Downloads\n",
    "rsync -r --progress seuge91@farm:~/fire/drep/vibrant_good/vibrant_good/_dRep/data_tables/genomeInformation.csv /mnt/c/Users/segeo/Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "~/fire/drep/blodgett/lukedrep\n",
    "\n",
    "# Dereplication alternative\n",
    "# concatenated file of all viral contigs\n",
    "# run megablast on all contigs (the one concatenated file) - raw results - for each contig, it'll run a blast comparison search for all the other contigs to show how similar they are\n",
    "# megablast is tolerant of gaps (not just pair-wise alignment that CDHIT does)\n",
    "# two python scripts - one calculates average nucleotide identity (anicalc.py), the other clusters (aniclust.py)\n",
    "# folder with file of interest, snakemake file, .sh, qsubfolder \"scripts\", scripts subfolder has anicalc.py and aniclust.py, \n",
    "# how is this different than drep? a little less sensitive?\n",
    "# leiden clustering algorithm - used for IMGVR v4 - mine NCBI for viral sequences - all sequenced viral sequences - clustered, all vOTUs\n",
    "# 327006 --> 222993\n",
    "-----------------------------------------------\n",
    "# Introduction\n",
    "\n",
    "# This stage combined all viral contigs previously identified in all samples \n",
    "# beginning with B or U into one file (BU.fna) and then used the clustering method used \n",
    "# in IMG/VR v4 to dereplicate the dataset.\n",
    "\n",
    "# Conda env\n",
    "# cluster-leiden - originally built to run snakemake and then had other dependencies added in\n",
    "\n",
    "# Files\n",
    "# BU.fna - combined fasta file input\n",
    "# BU_blast.tsv - BLAST results file\n",
    "# BU_ani.tsv - ANI results file\n",
    "# BU_clusters.tsv - cluster file\n",
    "# BU_clustered-leiden.fna - clustered fasta file output\n",
    "\n",
    "# cluster.sh - bash script that explicitly specifies settings used by IMG/VR v4/ accepted by community. \n",
    "# These are also generally the default values\n",
    "# clustering_slurm.sh - SLURM submission script. More cores = faster run time\n",
    "# contig-ani-leiden-clustering-pipeline.smk - snakefile that runs the pipeline\n",
    "# scripts/anicalc.py - Calculates ani values for results from megablast\n",
    "# scripts/aniclust.py - Performs clustering based on ani values. \n",
    "    # This step is slower than the algorithm used in IMG/VR v3 but reportedly more accurate (goes over my head a bit)\n",
    "\n",
    "# Dependencies\n",
    "snakemake\n",
    "numpy\n",
    "igraph\n",
    "BLAST\n",
    "seqkit\n",
    "\n",
    "# Notes\n",
    "\n",
    "# anicalc.py and aniclust.py need to be in a scripts folder within the folder containing the snakefile. \n",
    "# At some point I will look into making other locations possible\n",
    "\n",
    " \n",
    "\n",
    "# Refs\n",
    "# Github - https://github.com/apcamargo/bioinformatics-snakemake-pipelines/tree/main/contig-ani-leiden-clustering-pipeline\n",
    "# Paper - https://pubmed.ncbi.nlm.nih.gov/36399502/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir scripts\n",
    "nano anicalc.py\n",
    "-----------------------------------------------------------------------------------------\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "Hsp = namedtuple(\n",
    "    \"Hsp\",\n",
    "    [\"qname\", \"tname\", \"pid\", \"len\", \"qcoords\", \"tcoords\", \"evalue\", \"qlen\", \"tlen\"],\n",
    ")\n",
    "\n",
    "\n",
    "def parse_blast(handle):\n",
    "    for line in handle:\n",
    "        line = line.split()\n",
    "        yield Hsp(\n",
    "            qname=line[0],\n",
    "            tname=line[1],\n",
    "            pid=float(line[2]) / 100,\n",
    "            len=float(line[3]),\n",
    "            qcoords=sorted([int(line[4]), int(line[5])]),\n",
    "            tcoords=sorted([int(line[6]), int(line[7])]),\n",
    "            evalue=float(line[8]),\n",
    "            qlen=float(line[9]),\n",
    "            tlen=float(line[10]),\n",
    "        )\n",
    "\n",
    "\n",
    "def yield_alignment_blocks(handle):\n",
    "    # init block with 1st record\n",
    "    key, alns = None, None\n",
    "    for aln in parse_blast(handle):\n",
    "        if aln.qname == aln.tname:\n",
    "            continue\n",
    "        key = (aln.qname, aln.tname)\n",
    "        alns = [aln]\n",
    "        break\n",
    "    # loop over remaining records\n",
    "        for aln in parse_blast(handle):\n",
    "        # skip self hits\n",
    "        if aln.qname == aln.tname:\n",
    "            continue\n",
    "        # extend block\n",
    "        elif (aln.qname, aln.tname) == key:\n",
    "            alns.append(aln)\n",
    "        # yield block and start new one\n",
    "        else:\n",
    "            yield alns\n",
    "            key = (aln.qname, aln.tname)\n",
    "            alns = [aln]\n",
    "    yield alns\n",
    "\n",
    "\n",
    "def prune_alns(alns, max_evalue=snakemake.params.blast_max_evalue):\n",
    "    # remove short aligns\n",
    "    alns = [aln for aln in alns if aln.evalue <= max_evalue]\n",
    "    return alns\n",
    "\n",
    "\n",
    "def compute_ani(alns):\n",
    "    return round(sum(a.len * (a.pid) for a in alns) / sum(a.len for a in alns), 4)\n",
    "\n",
    "\n",
    "def compute_cov(alns):\n",
    "    # merge qcoords\n",
    "    coords = sorted([a.qcoords for a in alns])\n",
    "    nr_coords = [coords[0]]\n",
    "    for start, stop in coords[1:]:\n",
    "        # overlapping, update start coord\n",
    "        if start <= (nr_coords[-1][1] + 1):\n",
    "            nr_coords[-1][1] = max(nr_coords[-1][1], stop)\n",
    "        # non-overlapping, append to list\n",
    "                else:\n",
    "            nr_coords.append([start, stop])\n",
    "    # compute qry_cov\n",
    "    alen = sum([stop - start + 1 for start, stop in nr_coords])\n",
    "    qcov = round(alen / alns[0].qlen, 4)\n",
    "\n",
    "    # merge tcoords\n",
    "    coords = sorted([a.tcoords for a in alns])\n",
    "    nr_coords = [coords[0]]\n",
    "    for start, stop in coords[1:]:\n",
    "        # overlapping, update start coord\n",
    "        if start <= (nr_coords[-1][1] + 1):\n",
    "            nr_coords[-1][1] = max(nr_coords[-1][1], stop)\n",
    "        # non-overlapping, append to list\n",
    "        else:\n",
    "            nr_coords.append([start, stop])\n",
    "    # compute qry_cov\n",
    "    alen = sum(stop - start + 1 for start, stop in nr_coords)\n",
    "    tcov = round(alen / alns[0].tlen, 4)\n",
    "    return qcov, tcov\n",
    "\n",
    "\n",
    "out = open(snakemake.output[0], \"w\")\n",
    "out.write(\"\\t\".join([\"contig_1\", \"contig_2\", \"num_alns\", \"ani\", \"qcov\", \"tcov\"]) + \"\\n\")\n",
    "input = open(snakemake.input[0])\n",
    "for alns in yield_alignment_blocks(input):\n",
    "    alns = prune_alns(alns)\n",
    "    if len(alns) == 0:\n",
    "        continue\n",
    "    qname, tname = alns[0].qname, alns[0].tname\n",
    "    ani = compute_ani(alns)\n",
    "    qcov, tcov = compute_cov(alns)\n",
    "    row = [qname, tname, len(alns), ani, qcov, tcov]\n",
    "    out.write(\"\\t\".join(str(_) for _ in row) + \"\\n\")\n",
    "input.close()\n",
    "out.close()\n",
    "\n",
    "-----------------------------------------------------------------------------------------\n",
    "\n",
    "nano aniclust.py\n",
    "\n",
    "-----------------------------------------------------------------------------------------\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import itertools\n",
    "import random\n",
    "import igraph as ig\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Sequence:\n",
    "    def __init__(self, header, seq):\n",
    "        self._header = header\n",
    "        self._seq = seq\n",
    "\n",
    "    @property\n",
    "    def header(self):\n",
    "        return self._header\n",
    "\n",
    "    @property\n",
    "    def id(self):\n",
    "        return self._header.split()[0]\n",
    "\n",
    "    @property\n",
    "    def seq(self):\n",
    "        return self._seq\n",
    "\n",
    "    def count(self, substring):\n",
    "        return self.seq.count(substring)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.seq)\n",
    "\n",
    "\n",
    "def read_fasta(filepath):\n",
    "    with open(filepath) as fin:\n",
    "        last = None\n",
    "        while True:\n",
    "            if not last:\n",
    "                for l in fin:\n",
    "                    if l[0] == \">\":\n",
    "                        last = l[:-1]\n",
    "                        break\n",
    "            if not last:\n",
    "                break\n",
    "            name, seqs, last = last[1:], [], None\n",
    "            for l in fin:\n",
    "                if l[0] == \">\":\n",
    "                    last = l[:-1]\n",
    "                    break\n",
    "                seqs.append(l[:-1])\n",
    "            seqs = \"\".join(seqs)\n",
    "            if len(seqs):\n",
    "                yield Sequence(name, seqs)\n",
    "            if not last:\n",
    "                break\n",
    "\n",
    "\n",
    "def build_graph(\n",
    "    ani_file,\n",
    "    fasta_file,\n",
    "    min_ani=snakemake.params.min_ani,\n",
    "    min_cov=snakemake.params.min_cov,\n",
    "):\n",
    "    # Read the input FASTA file to get the names of all the sequences (nodes)\n",
    "    nodes = []\n",
    "    lengths = []\n",
    "    for seq in read_fasta(fasta_file):\n",
    "        nodes.append(seq.id)\n",
    "        lengths.append(len(seq) - seq.seq.upper().count(\"N\"))   \n",
    "    nodes_set = set(nodes)\n",
    "    # Initiate the lists that will store the edges between two sequences and\n",
    "    # their respective weights (ANI × coverage) and ANI\n",
    "    edges = []\n",
    "    weights = defaultdict(list)\n",
    "    anis = defaultdict(list)\n",
    "    with open(ani_file) as fin:\n",
    "        # Skip header\n",
    "        next(fin)\n",
    "        for line in fin:\n",
    "            (\n",
    "                seq_1,\n",
    "                seq_2,\n",
    "                _,\n",
    "                ani,\n",
    "                qcov,\n",
    "                tcov,\n",
    "            ) = line.strip().split(\"\\t\")\n",
    "            ani, qcov, tcov = (\n",
    "                float(ani),\n",
    "                float(qcov),\n",
    "                float(tcov),\n",
    "            )\n",
    "            # If both the sequences in the ANI pair were in the input FASTA file,\n",
    "            # their ANI is equal to or greater than `min_ani`, and either`qcov`\n",
    "            # or `tcov` is equal to or greater than `min_cov`, store their connection\n",
    "            if (\n",
    "                ((seq_1 in nodes_set) and (seq_2 in nodes_set))\n",
    "                and (ani >= min_ani)\n",
    "                and ((qcov >= min_cov) or (tcov >= min_cov))\n",
    "            ):\n",
    "                pair = tuple(sorted([seq_1, seq_2]))\n",
    "                edges.append(pair)\n",
    "                anis[pair].append(ani)\n",
    "                weight = ani * max([qcov, tcov])\n",
    "                weights[pair].append(weight)\n",
    "    # Take the mean ANI and weight for each pair of sequences\n",
    "    anis = [np.mean(anis[pair]) for pair in edges]\n",
    "    weights = [np.mean(weights[pair]) for pair in edges]\n",
    "    # Create a graph from the node list and add edges weighted by the ANI between\n",
    "    # the sequences being connected\n",
    "    graph = ig.Graph()\n",
    "    graph.add_vertices(list(nodes), attributes={\"length\": lengths})\n",
    "    graph.add_edges(edges, attributes={\"weight\": weights, \"ani\": weights})\n",
    "    return graph\n",
    "\n",
    "\n",
    "def pick_resolution(\n",
    "    graph,\n",
    "    target_avg_ani=snakemake.params.avg_ani,\n",
    "    steps=101,\n",
    "    seed=snakemake.params.seed,\n",
    "):\n",
    "    \"\"\"\n",
    "    Given a graph (`graph`) and a target average within-cluster ANI (`target_avg_ani`),\n",
    "    get the resolution parameter that will provide the closest within-community average\n",
    "    edge weight using the Leiden clustering algorithm.\n",
    "    \"\"\"\n",
    "    # The `last_res` variable will store the resolution (`res`) value of the previous\n",
    "    # iteration, while the `last_avg_weight` will store the difference between the average\n",
    "    # AAI of the previous iteration and the target average ANI (`target_avg_ani`)\n",
    "    last_res = None\n",
    "    last_avg_weight = None\n",
    "    # Iterate through resolution parameters (`res`)\n",
    "    for res in np.linspace(2, 0, steps):\n",
    "        random.seed(seed)\n",
    "        # Find the communities using the current resolution parameter\n",
    "        communities = graph.community_leiden(weights=\"weight\", resolution_parameter=res)\n",
    "        # Initiate the list that will store the edges' ANI\n",
    "        ani_list = []\n",
    "        # Iterate through the communities subgraphs ordered by length (largest → smallest)\n",
    "        for sg in reversed(np.argsort(communities.sizes())):\n",
    "            members = [i.attributes()[\"name\"] for i in communities.subgraph(sg).vs]\n",
    "            # If the community has a single member, break the loop\n",
    "            if len(members) < 2:\n",
    "                break\n",
    "            # Add all the pairwise ANI value to the `ani_list` list\n",
    "            for i, j in itertools.combinations(members, 2):\n",
    "                try:\n",
    "                    edge = graph.get_eid(i, j)\n",
    "                    edge = graph.es[edge]\n",
    "                    ani_list.append(edge.attributes()[\"ani\"])\n",
    "                # In case two nodes are not connected, skip the pair\n",
    "                except Exception:\n",
    "                    continue\n",
    "        # Compute the average value of all the edges in the iteration\n",
    "        current_avg_ani = np.mean(ani_list) if len(ani_list) else 1\n",
    "        # If this is the first iteration (that is, `res == 1`)\n",
    "        if res == 2:\n",
    "            last_res = res\n",
    "            last_avg_weight = current_avg_ani\n",
    "        # If the difference between the curent iteration average ANI and the target ANI\n",
    "        # is less than the difference of the previous iteration, store the value and keep\n",
    "        # iterating through `res`\n",
    "        elif np.abs(target_avg_ani - current_avg_ani) <= np.abs(\n",
    "            target_avg_ani - last_avg_weight\n",
    "        ):\n",
    "            last_res = res\n",
    "            last_avg_weight = current_avg_ani\n",
    "        # If the difference between the curent iteration average ANI and the target ANI\n",
    "        # is greater than the last iteration, break the loop\n",
    "        else:\n",
    "            break\n",
    "    return last_res\n",
    "\n",
    "\n",
    "ani_graph = build_graph(snakemake.input.ANI, snakemake.input.FASTA)\n",
    "if snakemake.params.leiden_resolution == \"auto\":\n",
    "    leiden_resolution = pick_resolution(ani_graph)\n",
    "else:\n",
    "    leiden_resolution = snakemake.params.leiden_resolution\n",
    "clusters = ani_graph.community_leiden(\n",
    "    weights=\"weight\", resolution_parameter=leiden_resolution\n",
    ")\n",
    "\n",
    "with open(snakemake.output[0], \"w\") as fout:\n",
    "    for i in reversed(np.argsort(clusters.sizes())):\n",
    "        subgraph = clusters.subgraph(i)\n",
    "        members = {v.attributes()[\"name\"]: v.attributes()[\"length\"] for v in subgraph.vs}\n",
    "        members = sorted(members, key=members.get, reverse=True)\n",
    "        fout.write(members[0] + \"\\t\" + \",\".join(members) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the directory structure for bowtie\n",
    "mkdir bowtie\n",
    "cd bowtie\n",
    "mkdir err log alignments ref coverm\n",
    "\n",
    "# concatenate all the dereplicated vOTUs into a single fasta file and save the new file in the bowtie folder.\n",
    "\n",
    "# the name of the new database file is all.drep.contigs.fa (updated_heat)\n",
    "\n",
    "cat /home/seuge91/heat/updated_drep/all/all_dRep/dereplicated_genomes/* >\n",
    "/home/seuge91/heat/updated_bowtie/all.drep.contigs.fa\n",
    "\n",
    "# the name of the new database file is all.good.drep.contigs.fa (fire)\n",
    "\n",
    "cat /home/seuge91/fire/drep/all_good/all_good/_dRep/dereplicated_genomes/* >\n",
    "/home/seuge91/fire/bowtie/all.good.drep.contigs.fa\n",
    "\n",
    "cat /home/seuge91/heat/drep/allgood/allgood_dRep/dereplicated_genomes/* > /home/seuge91/heat/bowtie/allgood.drep.contigs.fa\n",
    "\n",
    "# make bowtie reference script\n",
    "# \n",
    "cd /home/seuge91/fire/scripts\n",
    "nano bowtie_ref.sh\n",
    "\n",
    "cd /home/seuge91/heat/scripts\n",
    "nano updated_bowtie_ref.sh\n",
    "\n",
    "----------------------------\n",
    "\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=bt2ref\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH -t 12:00:00\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --partition=bmh\n",
    "\n",
    "# for calculating the amount of time the job takes\n",
    "begin=`date +%s`\n",
    "echo $HOSTNAME\n",
    "\n",
    "module load bowtie2\n",
    "\n",
    "cd /home/seuge91/fire/bowtie/ref/\n",
    "\n",
    "bowtie2-build ../all.good.drep.contigs.fa all_good_vibrant_drep\n",
    "\n",
    "# getting end time to calculate time elapsed\n",
    "end=`date +%s`\n",
    "elapsed=`expr $end - $begin`\n",
    "echo Time taken: $elapsed\n",
    "    \n",
    "----------------------------\n",
    "\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=bt2ref\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH -t 12:00:00\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --partition=bmh\n",
    "\n",
    "# for calculating the amount of time the job takes\n",
    "begin=`date +%s`\n",
    "echo $HOSTNAME\n",
    "\n",
    "module load bowtie2\n",
    "\n",
    "cd /home/seuge91/heat/bowtie/ref/\n",
    "\n",
    "bowtie2-build ../allgood.drep.contigs.fa allgood_vibrant_drep\n",
    "\n",
    "# getting end time to calculate time elapsed\n",
    "end=`date +%s`\n",
    "elapsed=`expr $end - $begin`\n",
    "echo Time taken: $elapsed\n",
    "    \n",
    "----------------------------\n",
    "\n",
    "# updated_bowtie_ref.sh\n",
    "----------------------------\n",
    "\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=bt2ref\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH -t 12:00:00\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --partition=bmh\n",
    "\n",
    "# for calculating the amount of time the job takes\n",
    "begin=`date +%s`\n",
    "echo $HOSTNAME\n",
    "\n",
    "module load bowtie2\n",
    "\n",
    "cd /home/seuge91/heat/updated_bowtie/ref/\n",
    "\n",
    "bowtie2-build ../all.drep.contigs.fa updated_all_vibrant_drep\n",
    "\n",
    "# getting end time to calculate time elapsed\n",
    "end=`date +%s`\n",
    "elapsed=`expr $end - $begin`\n",
    "echo Time taken: $elapsed\n",
    "    \n",
    "----------------------------\n",
    "\n",
    "# run the script to index the reference database\n",
    "cd /home/seuge91/fire/scripts\n",
    "sbatch --output=../bowtie/log/ref.log --error=../bowtie/err/ref.err bowtie_ref.sh\n",
    "\n",
    "cd /home/seuge91/fire/scripts\n",
    "sbatch --output=../lnu_bowtie/log/ref.log --error=../lnu_bowtie/err/ref.err lnu_bowtie_ref.sh\n",
    "\n",
    "cd /home/seuge91/fire/scripts\n",
    "sbatch --output=../burn_bowtie/log/ref.log --error=../burn_bowtie/err/ref.err burn_bowtie_ref.sh\n",
    "\n",
    "cd /home/seuge91/heat/scripts\n",
    "sbatch --output=../bowtie/log/ref.log --error=../bowtie/err/ref.err bowtie_ref.sh\n",
    "\n",
    "cd /home/seuge91/heat/scripts\n",
    "sbatch --output=../updated_bowtie/log/ref.log --error=../bowtie/err/ref.err updated_bowtie_ref.sh\n",
    "\n",
    "\n",
    "\n",
    "# map the reads against the database\n",
    "# make mapping script\n",
    "cd /home/seuge91/fire/scripts\n",
    "nano bowtie_map.sh\n",
    "\n",
    "cd /home/seuge91/heat/scripts\n",
    "nano updated_bowtie_map.sh\n",
    "\n",
    "----------------------------\n",
    "\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=bt2map\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH -t 2:00:00\n",
    "#SBATCH --ntasks=48\n",
    "#SBATCH --partition=bmh\n",
    "\n",
    "# for calculating the amount of time the job takes\n",
    "begin=`date +%s`\n",
    "echo $HOSTNAME\n",
    "\n",
    "#load modules\n",
    "module load bowtie2\n",
    "module load samtools\n",
    "\n",
    "path=/home/seuge91/heat/\n",
    "sample=${1}\n",
    "\n",
    "cd /home/seuge91/heat/updated_bowtie/ref/\n",
    "\n",
    "bowtie2 -x updated_all_vibrant_drep -p 48 \\\n",
    "-1 ${path}reads/rmphix/${sample}_R1_rmphix.fq.gz \\\n",
    "-2 ${path}reads/rmphix/${sample}_R2_rmphix.fq.gz \\\n",
    "-S ${path}updated_bowtie/alignments/${sample}.vib.sam \\\n",
    "--sensitive\n",
    "\n",
    "cd /home/seuge91/heat/updated_bowtie/alignments\n",
    "samtools view -F 4 -bS ${sample}.vib.sam | samtools sort > ${sample}.vib.sI.bam\n",
    "samtools index ${sample}.vib.sI.bam\n",
    "\n",
    "rm ${sample}.vib.sam\n",
    "\n",
    "# getting end time to calculate time elapsed\n",
    "end=`date +%s`\n",
    "elapsed=`expr $end - $begin`\n",
    "echo Time taken: $elapsed\n",
    "\n",
    "----------------------------\n",
    "\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=bt2map\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH -t 2:00:00\n",
    "#SBATCH --ntasks=48\n",
    "#SBATCH --partition=bmh\n",
    "\n",
    "# for calculating the amount of time the job takes\n",
    "begin=`date +%s`\n",
    "echo $HOSTNAME\n",
    "\n",
    "#load modules\n",
    "module load bowtie2\n",
    "module load samtools\n",
    "\n",
    "path=/home/seuge91/fire/\n",
    "sample=${1}\n",
    "\n",
    "cd /home/seuge91/fire/bowtie/ref/\n",
    "\n",
    "bowtie2 -x all_good_vibrant_drep -p 48 \\\n",
    "-1 ${path}reads/rmphix_combined/${sample}_R1_rmphix_combined.fq.gz \\\n",
    "-2 ${path}reads/rmphix_combined/${sample}_R2_rmphix_combined.fq.gz \\\n",
    "-S ${path}bowtie/alignments/${sample}.vib.sam \\\n",
    "--sensitive\n",
    "\n",
    "cd /home/seuge91/fire/bowtie/alignments\n",
    "samtools view -F 4 -bS ${sample}.vib.sam | samtools sort > ${sample}.vib.sI.bam\n",
    "samtools index ${sample}.vib.sI.bam\n",
    "\n",
    "rm ${sample}.vib.sam\n",
    "\n",
    "# getting end time to calculate time elapsed\n",
    "end=`date +%s`\n",
    "elapsed=`expr $end - $begin`\n",
    "echo Time taken: $elapsed\n",
    "    \n",
    "----------------------------\n",
    "\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=bt2map\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH -t 2:00:00\n",
    "#SBATCH --ntasks=48\n",
    "#SBATCH --partition=bmh\n",
    "\n",
    "# for calculating the amount of time the job takes\n",
    "begin=`date +%s`\n",
    "echo $HOSTNAME\n",
    "\n",
    "#load modules\n",
    "module load bowtie2\n",
    "module load samtools\n",
    "\n",
    "path=/home/seuge91/heat/\n",
    "sample=${1}\n",
    "\n",
    "cd /home/seuge91/heat/bowtie/ref/\n",
    "\n",
    "bowtie2 -x allgood_vibrant_drep -p 48 \\\n",
    "-1 ${path}reads/rmphix/${sample}_R1_rmphix.fq.gz \\\n",
    "-2 ${path}reads/rmphix/${sample}_R2_rmphix.fq.gz \\\n",
    "-S ${path}bowtie/alignments/${sample}.vib.sam \\\n",
    "--sensitive\n",
    "\n",
    "cd /home/seuge91/heat/bowtie/alignments\n",
    "samtools view -F 4 -bS ${sample}.vib.sam | samtools sort > ${sample}.vib.sI.bam\n",
    "samtools index ${sample}.vib.sI.bam\n",
    "\n",
    "rm ${sample}.vib.sam\n",
    "\n",
    "# getting end time to calculate time elapsed\n",
    "end=`date +%s`\n",
    "elapsed=`expr $end - $begin`\n",
    "echo Time taken: $elapsed\n",
    "    \n",
    "----------------------------\n",
    "\n",
    "# run mapping script\n",
    "cd /home/seuge91/fire/scripts\n",
    "\n",
    "for sample in $(<../sample.txt)\n",
    "do\n",
    "  sbatch --output=../bowtie/log/${sample}.map.log --error=../bowtie/err/${sample}.map.err bowtie_map.sh $sample\n",
    "done\n",
    "\n",
    "\n",
    "for sample in $(<../lnu_sample.txt)\n",
    "do\n",
    "  sbatch --output=../lnu_bowtie/log/${sample}.map.log --error=../lnu_bowtie/err/${sample}.map.err lnu_bowtie_map_tmp.sh $sample\n",
    "done\n",
    "\n",
    "for sample in $(<../burn_sample.txt)\n",
    "do\n",
    "  sbatch --output=../burn_bowtie/log/${sample}.map.log --error=../burn_bowtie/err/${sample}.map.err burn_bowtie_map.sh $sample\n",
    "done\n",
    "\n",
    "cd /home/seuge91/heat/scripts\n",
    "\n",
    "for sample in $(<../sampleIDs.txt)\n",
    "do\n",
    "  sbatch --output=../updated_bowtie/log/${sample}.map.log --error=../updated_bowtie/err/${sample}.map.err updated_bowtie_map.sh $sample\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate vOTU table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make coverM script\n",
    "nano coverm_tmp.sh\n",
    "nano coverm.sh\n",
    "\n",
    "nano updated_coverm.sh\n",
    "----------------------------\n",
    "\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=coverm\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH -t 10:00:00\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --partition=bmh\n",
    "\n",
    "# for calculating the amount of time the job takes\n",
    "begin=`date +%s`\n",
    "echo $HOSTNAME\n",
    "\n",
    "## you need to run the command from my coverm folder\n",
    "cd /home/csantosm/software/coverm-x86_64-unknown-linux-musl-0.6.1/\n",
    "\n",
    "path=/home/seuge91/heat/updated_bowtie/\n",
    "\n",
    "./coverm contig -m trimmed_mean --min-covered-fraction 0.75 -b ${path}/alignments/*.bam > ${path}/coverm/all.75.tmean.tsv\n",
    "\n",
    "# getting end time to calculate time elapsed\n",
    "end=`date +%s`\n",
    "elapsed=`expr $end - $begin`\n",
    "echo Time taken: $elapsed\n",
    "    \n",
    "----------------------------\n",
    "----------------------------\n",
    "\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=coverm\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH -t 10:00:00\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --partition=bmh\n",
    "\n",
    "# for calculating the amount of time the job takes\n",
    "begin=`date +%s`\n",
    "echo $HOSTNAME\n",
    "\n",
    "## you need to run the command from my coverm folder\n",
    "cd /home/csantosm/software/coverm-x86_64-unknown-linux-musl-0.6.1/\n",
    "\n",
    "path=/home/seuge91/fire/bowtie/\n",
    "\n",
    "./coverm contig -m trimmed_mean --min-covered-fraction 0.75 -b ${path}/alignments/*.bam > ${path}/coverm/all.good.75.tmean.tsv\n",
    "\n",
    "# getting end time to calculate time elapsed\n",
    "end=`date +%s`\n",
    "elapsed=`expr $end - $begin`\n",
    "echo Time taken: $elapsed\n",
    "    \n",
    "----------------------------\n",
    "\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=coverm\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH -t 10:00:00\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --partition=bmh\n",
    "\n",
    "# for calculating the amount of time the job takes\n",
    "begin=`date +%s`\n",
    "echo $HOSTNAME\n",
    "\n",
    "## you need to run the command from my coverm folder\n",
    "cd /home/csantosm/software/coverm-x86_64-unknown-linux-musl-0.6.1/\n",
    "\n",
    "path=/home/seuge91/heat/bowtie/\n",
    "\n",
    "./coverm contig -m trimmed_mean --min-covered-fraction 0.75 -b ${path}/alignments/*.bam > ${path}/coverm/allgood.75.tmean.tsv\n",
    "\n",
    "# getting end time to calculate time elapsed\n",
    "end=`date +%s`\n",
    "elapsed=`expr $end - $begin`\n",
    "echo Time taken: $elapsed\n",
    "    \n",
    "----------------------------\n",
    "\n",
    "# run script\n",
    "cd /home/seuge91/fire/scripts\n",
    "sbatch --output=../bowtie/log/coverm.log --output=../bowtie/err/coverm.err coverm_tmp.sh\n",
    "\n",
    "cd /home/seuge91/fire/scripts\n",
    "sbatch --output=../lnu_bowtie/log/coverm.log --output=../lnu_bowtie/err/coverm.err lnu_coverm_tmp.sh\n",
    "\n",
    "cd /home/seuge91/fire/scripts\n",
    "sbatch --output=../burn_bowtie/log/coverm.log --output=../burn_bowtie/err/coverm.err burn_coverm.sh\n",
    "\n",
    "cd /home/seuge91/heat/scripts\n",
    "sbatch --output=../bowtie/log/coverm.log --output=../bowtie/err/coverm.err coverm.sh\n",
    "\n",
    "cd /home/seuge91/heat/scripts\n",
    "sbatch --output=../updated_bowtie/log/coverm.log --output=../updated_bowtie/err/coverm.err updated_coverm.sh\n",
    "\n",
    "# download table\n",
    "\n",
    "scp seuge91@farm.cse.ucdavis.edu:/home/seuge91/fire/lnu_bowtie/coverm/*.tsv /mnt/c/Users/segeo/Downloads\n",
    "\n",
    "scp seuge91@farm.cse.ucdavis.edu:/home/seuge91/fire/burn_bowtie/coverm/*.tsv /mnt/c/Users/segeo/Downloads\n",
    "\n",
    "scp seuge91@farm.cse.ucdavis.edu:/home/seuge91/heat/bowtie/coverm/*.tsv /mnt/c/Users/segeo/Downloads\n",
    "\n",
    "scp seuge91@farm.cse.ucdavis.edu:/home/seuge91/heat/updated_bowtie/coverm/*.tsv /mnt/c/Users/segeo/Downloads\n",
    "\n",
    "\n",
    "# OK THIS PART STARTS TO GET SKETCHY...\n",
    "\n",
    "\n",
    "# download table\n",
    "rsync -r --progress seuge91@farm:~/blodgett/Project_JESG_Nova348P_Geonczy/bowtie2/sam/covm.75.tmean.tsv /mnt/c/Users/segeo/Downloads\n",
    "\n",
    "    \n",
    "cd /home/seuge91/fire/reads/\n",
    "grep -e '#Total' *gz_stats.txt > cleanreadsbio.txt\n",
    "mv cleanreadsbio.txt /home/jdfudyma/2203_pH/sam\n",
    "\n",
    "cd /home/jdfudyma/2203_pH/remove_phix\n",
    "grep -e '#Total' *gz_stats.txt > cleanreadstech.txt\n",
    "mv cleanreadsbio.txt /home/jdfudyma/2203_pH/sam\n",
    "```\n",
    "\n",
    "6) Download coverage tables to your personal computer\n",
    "```{bash}\n",
    "#use scp to transfer\n",
    "#make sure you ARE IN BASE TERMINAL (i.e. Janes-MBP) not logged into farm\n",
    "\n",
    "#path to things on farm, path to where on your computer you want things \n",
    "\n",
    "#getting coverage tables\n",
    "scp jdfudyma@farm.cse.ucdavis.edu:/home/jdfudyma/2203_pH/sam/*.tsv ~/Documents/CA/UC_Davis/Emerson_Lab/21.pH\n",
    "\n",
    "#getting clean reads files\n",
    "scp jdfudyma@farm.cse.ucdavis.edu:/home/jdfudyma/2203_pH/sam/*.txt ~/Documents/CA/UC_Davis/Emerson_Lab/21.pH\n",
    "    \n",
    "scp seuge91@farm.cse.ucdavis.edu:/home/seuge91/fire/lnu_bowtie/coverm/*.tsv /mnt/c/Users/segeo/Downloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Count Table for Differential Abundance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make covermcount script\n",
    "nano updated_covermcount.sh\n",
    "\n",
    "----------------------------\n",
    "\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=covermcount\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH -t 10:00:00\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --partition=bmh\n",
    "\n",
    "# for calculating the amount of time the job takes\n",
    "begin=`date +%s`\n",
    "echo $HOSTNAME\n",
    "\n",
    "## you need to run the command from my coverm folder\n",
    "cd /home/csantosm/software/coverm-x86_64-unknown-linux-musl-0.6.1/\n",
    "\n",
    "path=/home/seuge91/heat/updated_bowtie/\n",
    "\n",
    "./coverm contig -m count -b ${path}/alignments/*.bam > ${path}/coverm/all.count.tsv\n",
    "\n",
    "# getting end time to calculate time elapsed\n",
    "end=`date +%s`\n",
    "elapsed=`expr $end - $begin`\n",
    "echo Time taken: $elapsed\n",
    "    \n",
    "----------------------------\n",
    "\n",
    "# make covermcount script\n",
    "nano covermcount.sh\n",
    "\n",
    "----------------------------\n",
    "\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=covermcount\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH -t 10:00:00\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --partition=bmh\n",
    "\n",
    "# for calculating the amount of time the job takes\n",
    "begin=`date +%s`\n",
    "echo $HOSTNAME\n",
    "\n",
    "## you need to run the command from my coverm folder\n",
    "cd /home/csantosm/software/coverm-x86_64-unknown-linux-musl-0.6.1/\n",
    "\n",
    "path=/home/seuge91/heat/bowtie/\n",
    "\n",
    "./coverm contig -m count -b ${path}/alignments/*.bam > ${path}/coverm/allgood.count.tsv\n",
    "\n",
    "# getting end time to calculate time elapsed\n",
    "end=`date +%s`\n",
    "elapsed=`expr $end - $begin`\n",
    "echo Time taken: $elapsed\n",
    "    \n",
    "----------------------------\n",
    "    \n",
    "# Download table\n",
    "scp seuge91@farm.cse.ucdavis.edu:/home/seuge91/heat/bowtie/coverm/*.count.tsv /mnt/c/Users/segeo/Downloads\n",
    "    \n",
    "scp seuge91@farm.cse.ucdavis.edu:/home/seuge91/heat/bowtie/coverm/*.count.tsv /mnt/c/Users/segeo/Downloads\n",
    "    \n",
    "scp seuge91@farm.cse.ucdavis.edu:/home/seuge91/heat/updated_bowtie/coverm/*.count.tsv /mnt/c/Users/segeo/Downloads\n",
    "    \n",
    "    \n",
    "scp /mnt/d/MT_6123968/* seuge91@farm.cse.ucdavis.edu:/home/seuge91/jane \n",
    "    \n",
    "scp /mnt/d/MT_6123968 jdfudyma@farm.cse.ucdavis.edu:/home/jdfudyma/2305_spatial/metabolomes \n",
    "    \n",
    "chmod777 jane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#SBATCH --job-name=coverm\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH -t 10:00:00\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --partition=high2\n",
    "\n",
    "# for calculating the amount of time the job takes\n",
    "begin=`date +%s`\n",
    "echo $HOSTNAME\n",
    "\n",
    "## you need to run the command from my coverm folder\n",
    "cd /home/csantosm/software/coverm-x86_64-unknown-linux-musl-0.6.1/\n",
    "\n",
    "path=/home/seuge91/fire/lnu_bowtie/\n",
    "\n",
    "./coverm contig -m trimmed_mean --min-covered-fraction 0.75 -b ${path}/alignments/*.bam > ${path}/coverm/lnu.good.75.tmean.tsv\n",
    "\n",
    "# getting end time to calculate time elapsed\n",
    "end=`date +%s`\n",
    "elapsed=`expr $end - $begin`\n",
    "echo Time taken: $elapsed\n",
    "\n",
    "----------------------------\n",
    "\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=coverm\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH -t 10:00:00\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --partition=high2\n",
    "\n",
    "# for calculating the amount of time the job takes\n",
    "begin=`date +%s`\n",
    "echo $HOSTNAME\n",
    "\n",
    "## you need to run the command from my coverm folder\n",
    "cd /home/csantosm/software/coverm-x86_64-unknown-linux-musl-0.6.1/\n",
    "squeu\n",
    "path=/home/seuge91/fire/burn_bowtie/\n",
    "\n",
    "./coverm contig -m trimmed_mean --min-covered-fraction 0.75 -b ${path}/alignments/*.bam > ${path}/coverm/burn.good.75.tmean.tsv\n",
    "\n",
    "# getting end time to calculate time elapsed\n",
    "end=`date +%s`\n",
    "elapsed=`expr $end - $begin`\n",
    "echo Time taken: $elapsed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Host Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#SBATCH -D /home/seuge91/heat/scripts/\n",
    "#SBATCH --job-name=iphop_split\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH -t 1:00:00\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --partition=bmm\n",
    "\n",
    "# for calculating the amount of time the job takes\n",
    "begin=`date +%s`\n",
    "echo $HOSTNAME\n",
    "\n",
    "source /home/csantosm/initconda\n",
    "conda activate IPHOP\n",
    "\n",
    "set=${1}\n",
    "\n",
    "full_db=/home/seuge91/heat/updated_bowtie/all.drep.contigs.fa\n",
    "split_db=/home/seuge91/heat/iphop/split_db\n",
    "\n",
    "iphop split --input_file ${full_db} --split_dir ${split_db}\n",
    "\n",
    "#getting end time to calculate time elapsed\n",
    "end=`date +%s`\n",
    "elapsed=`expr $end - $begin`\n",
    "echo Time taken: $elapsed\n",
    "\n",
    "----------------------------\n",
    "\n",
    "#!/bin/bash\n",
    "#SBATCH -D /home/seuge91/heat/scripts/\n",
    "#SBATCH --job-name=iphop_predict\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH -t 24:00:00\n",
    "#SBATCH --mem=128GB\n",
    "#SBATCH --ntasks=24\n",
    "#SBATCH --partition=bmm\n",
    "\n",
    "# for calculating the amount of time the job takes\n",
    "begin=`date +%s`\n",
    "echo $HOSTNAME\n",
    "\n",
    "source /home/csantosm/initconda\n",
    "conda activate IPHOP\n",
    "\n",
    "batch=${1}\n",
    "\n",
    "iphop_db=/home/csantosm/databases/IPHOP_db/Sept_2021_pub\n",
    "\n",
    "cd /home/seuge91/heat/iphop\n",
    "\n",
    "iphop predict --fa_file ./split_db/${batch}.fna --out_dir ./results/${batch} --db_dir ${iphop_db} --num_threads 24\n",
    "\n",
    "#getting end time to calculate time elapsed\n",
    "end=`date +%s`\n",
    "elapsed=`expr $end - $begin`\n",
    "echo Time taken: $elapsed\n",
    "    \n",
    "----------------------------\n",
    "    \n",
    "cd /home/seuge91/heat/scripts\n",
    "\n",
    "batches=\"batch_00001 batch_00002 batch_00003 batch_00004 batch_00005 batch_00006 batch_00007 batch_00008 batch_00009 batch_00010 batch_00011 batch_00012 batch_00013 batch_00014 batch_00015 batch_00016 batch_00017 batch_00018\"\n",
    "for batch in $batches\n",
    "do\n",
    "sbatch --output=../iphop/log/${batch}.log --error=../iphop/err/${batch}.err iphop_predict.sh $batch\n",
    "done\n",
    "\n",
    "batches=\"batch_00000\"\n",
    "for batch in $batches\n",
    "do\n",
    "sbatch --output=../iphop/log/${batch}.log --error=../iphop/err/${batch}.err iphop_predict.sh $batch\n",
    "done\n",
    "\n",
    "# combine results from batches\n",
    "# get header from one of the batches\n",
    "cd batch_00000\n",
    "head -n 1 Host_prediction_to_genome_m90.csv > all_Host_prediction_to_genome_m90.csv\n",
    "head -n 10 Detailed_output_by_tool.csv > all_Detailed_output_by_tool.csv\n",
    "# move files up a folder\n",
    "mv all_Host_prediction_to_genome_m90.csv ..\n",
    "#get body of each batch and concatenate to main file\n",
    "tail -n +2 -q batch_*/Host_prediction_to_genome_m90.csv >> all_Host_prediction_to_genome_m90.csv\n",
    "tail -n +2 -q batch_*/Host_prediction_to_genus_m90.csv >> all_Host_prediction_to_genus_m90.csv\n",
    "tail -n +11 batch_*/Detailed_output_by_tool.csv >> all_Detailed_output_by_tool.csv\n",
    "\n",
    "# download files\n",
    "scp seuge91@farm.cse.ucdavis.edu:/home/seuge91/heat/iphop/results/*.csv /mnt/c/Users/segeo/Downloads\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vcontact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make vcontact directory for vibrant contig proteins!!\n",
    "mkdir vcontact\n",
    "\n",
    "# within vcontact directory, make more directories\n",
    "mkdir err log\n",
    "\n",
    "# concatenate all viral contigs from drep into a single file\n",
    "cat *.fa > allcontigs.drep.fa \n",
    "\n",
    "# count how many total contigs\n",
    "grep -c contig allcontigs.drep.fa\n",
    "#18,869\n",
    "\n",
    "# move file into vcontact directory\n",
    "mv allcontigs.drep.fa ../../../vcontact\n",
    "\n",
    "# predict proteins \n",
    "#-i flag means input file, all contigs from drep \n",
    "#-a flag means output file that is protein, needs to be named with end of \".faa\" \n",
    " \n",
    "source /home/csantosm/inticonda \n",
    "conda activate PRODIGAL \n",
    "prodigal -i allcontigs.drep.fa -a votus.prodigal.faa -p meta\n",
    "\n",
    "\n",
    "# create gene2genome.csv file\n",
    "# make script\n",
    "nano gtg.sh\n",
    "\n",
    "----------------------------\n",
    "\n",
    "#!/bin/bash\n",
    "#SBATCH -D /home/seuge91/heat/scripts/\n",
    "#SBATCH --job-name=g2g\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH -t 1:00:00\n",
    "#SBATCH --partition=bmm\n",
    "\n",
    "# for calculating the amount of time the job takes\n",
    "begin=`date +%s`\n",
    "echo $HOSTNAME\n",
    "\n",
    "source /home/csantosm/initconda\n",
    "conda activate VCONTACT2\n",
    "\n",
    "cd /home/seuge91/heat/vcontact/\n",
    "\n",
    "vcontact2_gene2genome -p ./votus.prodigal.faa  \\\n",
    "-o ./votus.gene2genome.csv \\\n",
    "-s Prodigal-FAA\n",
    "\n",
    "# getting end time to calculate time elapsed\n",
    "end=`date +%s`\n",
    "elapsed=`expr $end - $begin`\n",
    "echo Time taken: $elapsed\n",
    "\n",
    "----------------------------\n",
    "\n",
    "# run script\n",
    "sbatch --output=../vcontact/log/g2g.log --error=../vcontact/err/g2g.err g2g.sh\n",
    "\n",
    "# create vcontact script\n",
    "cd /home/seuge91/heat/scripts/\n",
    "nano vcontact\n",
    "\n",
    "----------------------------\n",
    "\n",
    "#!/bin/bash\n",
    "#SBATCH -D /home/seuge91/heat/scripts/\n",
    "#SBATCH --job-name=vcontact\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH -t 48:00:00\n",
    "#SBATCH --ntasks=16\n",
    "#SBATCH --partition=bmm\n",
    "\n",
    "# for calculating the amount of time the job takes\n",
    "begin=`date +%s`\n",
    "echo $HOSTNAME\n",
    "\n",
    "source /home/csantosm/initconda\n",
    "conda activate VCONTACT2\n",
    "module load deprecated/java\n",
    "\n",
    "cd /home/seuge91/heat/vcontact/\n",
    "\n",
    "vcontact2 --raw-proteins votus.prodigal.faa \\\n",
    "--rel-mode 'Diamond' \\\n",
    "--db 'ProkaryoticViralRefSeq85-Merged' \\\n",
    "--proteins-fp votus.gene2genome.csv \\\n",
    "--pcs-mode MCL \\\n",
    "--vcs-mode ClusterONE \\\n",
    "--threads 16 \\\n",
    "--c1-bin /home/csantosm/miniconda3/bin/cluster_one-1.0.jar \\\n",
    "--output-dir vcontact_out\n",
    "\n",
    "# getting end time to calculate time elapsed\n",
    "end=`date +%s`\n",
    "elapsed=`expr $end - $begin`\n",
    "echo Time taken: $elapsed\n",
    "    \n",
    "----------------------------\n",
    "\n",
    "# run script\n",
    "sbatch --output=../vcontact/log/vc2.log --error=../vcontact/err/vc2.err vcontact.sh\n",
    "\n",
    "# download files\n",
    "scp seuge91@farm.cse.ucdavis.edu:/home/seuge91/heat/vcontact/vcontact_out/genome_by_genome_overview.csv /mnt/c/Users/segeo/Downloads\n",
    "scp seuge91@farm.cse.ucdavis.edu:/home/seuge91/heat/vcontact/vcontact_out/c1.ntw /mnt/c/Users/segeo/Downloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Removing intermediate files\n",
    "\n",
    "# remove trimmed reads (anything before Phi X removal)\n",
    "rm -r trimmed\n",
    "rm -r unpaired\n",
    "\n",
    "# if you have concatenated batch fastq.gz files, remove all the batches and just keep the combined concatenated files\n",
    "rm -r rmphix\n",
    "rm -r rmphix_unpaired\n",
    "\n",
    "# KEEP rmphix_combined and rmphix_combined_unpaired\n",
    "\n",
    "# remove intermediate files in megahit\n",
    "cd /megahit\n",
    "rm -r *V*/intermediate_contigs\n",
    "\n",
    "\n",
    "\n",
    "# remove Mdb.csv in dereplication step\n",
    "cd ../drep/all/all_dRep/data_tables\n",
    "\n",
    "# explanation of what each file represents\n",
    "cd vibrant_good/vibrant_good_dRep/data_tables\n",
    "./data_tables\n",
    "...../Bdb.csv  # Sequence locations and filenames\n",
    "...../Cdb.csv  # Genomes and cluster designations\n",
    "...../Chdb.csv # CheckM results for Bdb\n",
    "...../Mdb.csv  # Raw results of MASH comparisons\n",
    "...../Ndb.csv  # Raw results of ANIn comparisons\n",
    "...../Sdb.csv  # Scoring information\n",
    "...../Wdb.csv  # Winning genomes\n",
    "...../Widb.csv # Winning genomes' checkM information\n",
    "\n",
    "rm Mdb.csv\n",
    "\n",
    "# SAM files (read mapping, bowtie)\n",
    "\n",
    "\n",
    "# how much storage you are taking up\n",
    "# make script\n",
    "\n",
    "nano diskuse.sb\n",
    "\n",
    "-----------------------------------------------------------------------------------------\n",
    "\n",
    "#!/bin/bash\n",
    "#SBATCH -D /home/seuge91\n",
    "#SBATCH --job-name=DiskUse\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH -t 12:00:00\n",
    "#SBATCH --ntasks=8\n",
    "#SBATCH --output=DiskUse_%j.out\n",
    "#SBATCH --error=DiskUse_%j.err\n",
    "#SBATCH --partition=med2\n",
    "\n",
    "cd /home/seuge91\n",
    "echo seuge91 >> /home/seuge91/DiskUse.txt\n",
    "du -sh . >> /home/seuge91/DiskUse.txt\n",
    "\n",
    "-----------------------------------------------------------------------------------------\n",
    "\n",
    "sbatch diskuse.sb\n",
    "\n",
    "# will create DiskUse.txt file and show how many TB you are using"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing dataframe in R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tidyverse)\n",
    "otu=read_delim(\"lnu.good.75.tmean.tsv\", delim = \"\\t\", col_names = TRUE)\n",
    "contignames = otu$Contig\n",
    "\n",
    "otu=otu%>%\n",
    "  select(-Contig)\n",
    "otu=data.frame(otu)\n",
    "row.names(otu)=contignames\n",
    "colnames(otu) <- gsub(pattern=\".vib.sI.Trimmed.Mean\", replacement = \"\", x=colnames(otu))\n",
    "\n",
    "library(vegan)\n",
    "\n",
    "# relativized data set\n",
    "# took every cell (OTUs abundance in particular sample) and divided it by the sum of every OTUs abundance in that sample = to give relative abundance\n",
    "# decostand - data (otu table), type of standardization/relativization (total is relative abundance), margin says if we perform that across rows are column and 2 is equal to column\n",
    "# colSums confirms that everything equals 1\n",
    "\n",
    "otu.rel=decostand(otu, method = \"total\", MARGIN = 2)\n",
    "colSums(otu.rel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCoA plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate pairwise distance between each pair of samples\n",
    "# in a particular sample, how similar is this community to all of the other samples?\n",
    "\n",
    "# bray-curtis takes into account both presence and relative abundance\n",
    "# make a distance object\n",
    "# t(otu.rel) because vegdist by default calculate pairwise distances between rows, but we wanted columns because this represents samples, so t transposes so that columns are rows\n",
    "\n",
    "otu.dist=vegdist(t(otu.rel), method = \"bray\")\n",
    "\n",
    "# distance matrix tells us how dissimilar every pair of samples are from each other\n",
    "# this matrix has 38 dimensions of data represented, how far apart every samples is from every other sample\n",
    "# so PCoA, collapses 38 axes into 2 axes to visualize 38 dimensions into 2 dimensions\n",
    "\n",
    "otu.pcoa=cmdscale(otu.dist, eig = TRUE)\n",
    "otu.pcoa$points\n",
    "otu.pcoa.points=data.frame(otu.pcoa$points)\n",
    "colnames(otu.pcoa.points)=c(\"pcoa1\", \"pcoa2\")\n",
    "\n",
    "otu.pcoa.points$location <- \"McLaughlin\"\n",
    "otu.pcoa.points$location[grepl(\"QV\", row.names(otu.pcoa.points))] <- \"QuailRidge\"\n",
    "\n",
    "# make a PCoA plot!!\n",
    "\n",
    "ggplot(otu.pcoa.points, aes(x=pcoa1, y=pcoa2, color=location))+\n",
    "  geom_point()\n",
    "    \n",
    "# make another file with one column in common with name the same\n",
    "# sample 1 is called the exact same way in both tables and you can join both dataframes by that column\n",
    "\n",
    "write.table(x=otu.pcoa.points, file = \"lnu_pcoa.tsv\", quote=FALSE, sep=\"\\t\" )\n",
    "\n",
    "# calculate variance\n",
    "otu.pcoa=cmdscale(otu.dist, eig = TRUE)\n",
    "otu.pcoa$eig[1]/sum(otu.pcoa$eig)\n",
    "otu.pcoa$eig[2]/sum(otu.pcoa$eig)\n",
    "\n",
    "# upload mapping data (metadata)\n",
    "map=read_delim(\"burn_pcoa_metadata.txt\", delim = \"\\t\", col_names = TRUE)\n",
    "ggplot(map, aes(x=pcoa1, y=pcoa2, color=Burned, shape=Plot))+\n",
    "  geom_point()\n",
    "    \n",
    "# PERMANOVA\n",
    "adonis(otu.dist\n",
    "~map$'variableofinterest')\n",
    "\n",
    "# p-value - probability that the result would happen by random chance, significance \n",
    "# R^2 - how close of an association \n",
    "# pseudo F - slope of line\n",
    "\n",
    "# if multiple variables are significant, the one with higher F. model may be contributing more to the differences"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
